train_baseline:
#!/usr/bin/env python3
import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, Subset
from torch.utils.data.sampler import WeightedRandomSampler
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import f1_score
import random
from datetime import datetime
import time
import torch.nn.functional as F
import json
from PIL import Image
import torchvision.transforms as T
from shapely import wkt
import torchvision.models as models

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.append(project_root)

# Maps the xBD "subtype" to an integer label.
DAMAGE_LABELS = {
    "no-damage": 0,
    "minor-damage": 1,
    "major-damage": 2,
    "destroyed": 3
}

#-------------------------------------------------------
# MODEL DEFINITION
#-------------------------------------------------------
class PostDisasterModel(nn.Module):
    """
    Simplified model that only takes post-disaster imagery as input
    """
    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.5):
        super(PostDisasterModel, self).__init__()
        
        # Use a more powerful backbone (ResNet50)
        try:
            # For torch 1.13+
            base_model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)
        except TypeError:
            # For older torch versions
            base_model = models.resnet50(pretrained=pretrained)
        
        # Extract feature layers from ResNet50
        self.backbone = nn.Sequential(*list(base_model.children())[:-2])
        
        # Get the number of features from the backbone
        self.feature_dim = base_model.fc.in_features  # 2048 for ResNet50
        
        # Global average pooling to reduce spatial dimensions
        self.gap = nn.AdaptiveAvgPool2d(1)
        
        # MLP head with dropout for better generalization
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
        
        # Initialize weights of the classifier
        self._initialize_weights(self.classifier)
        
    def _initialize_weights(self, module):
        for m in module.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                
    def forward(self, post_images):
        """
        Forward pass with only post-disaster images
        """
        # Extract features from post-disaster images
        features = self.backbone(post_images)
        
        # Global average pooling
        pooled_features = self.gap(features).view(-1, self.feature_dim)
        
        # Classification
        output = self.classifier(pooled_features)
        
        return output

#-------------------------------------------------------
# DATASET DEFINITION
#-------------------------------------------------------
class XBDPostDisasterDataset(Dataset):
    """
    Dataset that only loads post-disaster imagery:
      - Scans the dataset directory.
      - In hierarchical mode (flat_structure=False): expects subfolders per disaster (each with an 'images' and 'labels' folder).
      - In flat mode (flat_structure=True): expects root_dir to contain 'images' and 'labels' folders directly.
      - For each post-disaster JSON, enumerates building polygons, computes the bounding box, and crops the post images.
      - Returns (post_patch, label) for each building.
    """
    def __init__(self,
                 root_dir,
                 crop_size=224,
                 max_samples=None,
                 flat_structure=False,
                 augment=False):
        """
        :param root_dir: Directory where the data is stored.
           - If flat_structure is False: root_dir should contain subfolders for each disaster.
           - If True: root_dir should contain 'images' and 'labels' folders directly.
        :param crop_size: Crop size for post-disaster images.
        :param max_samples: Optional limit on the number of samples.
        :param flat_structure: Whether the folder structure is flat.
        :param augment: If True, applies data augmentation (random horizontal flip and random rotation).
        """
        super().__init__()
        self.root_dir = root_dir
        self.crop_size = crop_size
        self.max_samples = max_samples
        self.flat_structure = flat_structure
        self.augment = augment
        self.coord_key = "xy"  # Fixed to use xy coordinates
        
        # Define transforms
        base_transforms = [
            T.Resize((crop_size, crop_size)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
        ]
        
        if self.augment:
            self.transform = T.Compose([
                T.RandomHorizontalFlip(),
                T.RandomRotation(10),
                *base_transforms
            ])
        else:
            self.transform = T.Compose(base_transforms)
        
        self.samples = self._gather_samples()
        if self.max_samples is not None and len(self.samples) > self.max_samples:
            self.samples = random.sample(self.samples, self.max_samples)

    def _gather_samples(self):
        samples = []
        
        # Check if we have a flat structure (test/ directory)
        if self.flat_structure:
            images_dir = os.path.join(self.root_dir, "images")
            labels_dir = os.path.join(self.root_dir, "labels")
            
            if os.path.isdir(images_dir) and os.path.isdir(labels_dir):
                print(f"Processing flat structure at {self.root_dir}")
                
                # Find all post disaster JSONs
                label_files = [f for f in os.listdir(labels_dir) if f.endswith("_post_disaster.json")]
                
                for label_file in label_files:
                    base_id = label_file.replace("_post_disaster.json", "")
                    post_img_name = base_id + "_post_disaster.png"
                    post_json_path = os.path.join(labels_dir, label_file)
                    post_img_path = os.path.join(images_dir, post_img_name)
                    
                    if not (os.path.isfile(post_json_path) and os.path.isfile(post_img_path)):
                        continue
                        
                    with open(post_json_path, 'r') as f:
                        post_data = json.load(f)
                        
                    # Get building features with xy coordinates - FIXED
                    feats = post_data.get("features", {}).get(self.coord_key, [])
                    for feat in feats:
                        damage_type = feat.get("properties", {}).get("subtype", "").lower()
                        if damage_type not in DAMAGE_LABELS:
                            continue
                            
                        label = DAMAGE_LABELS[damage_type]
                        wkt_str = feat.get("wkt", None)
                        if wkt_str is None:
                            continue
                            
                        polygon = wkt.loads(wkt_str)
                        minx, miny, maxx, maxy = polygon.bounds
                        samples.append({
                            "post_img": post_img_path,
                            "bbox": (minx, miny, maxx, maxy),
                            "label": label
                        })
        else:
            # Hierarchical structure: root_dir contains subfolders per disaster
            try:
                disasters = [d for d in os.listdir(self.root_dir)
                            if os.path.isdir(os.path.join(self.root_dir, d))
                            and d.lower() != "spacenet_gt"]
                
                print(f"Found {len(disasters)} disaster folders")
                
                for disaster in disasters:
                    disaster_dir = os.path.join(self.root_dir, disaster)
                    images_dir = os.path.join(disaster_dir, "images")
                    labels_dir = os.path.join(disaster_dir, "labels")
                    
                    if not (os.path.isdir(images_dir) and os.path.isdir(labels_dir)):
                        print(f"Warning: Missing images or labels directory for disaster: {disaster}")
                        continue
                        
                    # Only look for post-disaster files
                    label_files = [f for f in os.listdir(labels_dir) if f.endswith("_post_disaster.json")]
                    print(f"Disaster {disaster}: Found {len(label_files)} post-disaster label files")
                    
                    for label_file in label_files:
                        base_id = label_file.replace("_post_disaster.json", "")
                        post_img_name = base_id + "_post_disaster.png"
                        post_json_path = os.path.join(labels_dir, label_file)
                        post_img_path = os.path.join(images_dir, post_img_name)
                        
                        if not (os.path.isfile(post_json_path) and os.path.isfile(post_img_path)):
                            continue
                            
                        with open(post_json_path, 'r') as f:
                            post_data = json.load(f)
                            
                        # Get building features with xy coordinates - FIXED
                        feats = post_data.get("features", {}).get(self.coord_key, [])
                        for feat in feats:
                            damage_type = feat.get("properties", {}).get("subtype", "").lower()
                            if damage_type not in DAMAGE_LABELS:
                                continue
                                
                            label = DAMAGE_LABELS[damage_type]
                            wkt_str = feat.get("wkt", None)
                            if wkt_str is None:
                                continue
                                
                            polygon = wkt.loads(wkt_str)
                            minx, miny, maxx, maxy = polygon.bounds
                            samples.append({
                                "post_img": post_img_path,
                                "bbox": (minx, miny, maxx, maxy),
                                "label": label,
                                "disaster": disaster  # Track which disaster this is from
                            })
            except Exception as e:
                print(f"Error gathering samples: {e}")
                
        print(f"Total gathered samples: {len(samples)}")
        
        # Check class distribution
        labels = [s["label"] for s in samples]
        unique_labels, counts = np.unique(labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = [name for name, idx in DAMAGE_LABELS.items() if idx == label][0]
            print(f"  {class_name}: {count} samples ({count/len(samples)*100:.2f}%)")
            
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        post_path = item["post_img"]
        (minx, miny, maxx, maxy) = item["bbox"]
        label = item["label"]

        try:
            post_img = Image.open(post_path).convert("RGB")
            post_crop = self._center_crop(post_img, minx, miny, maxx, maxy, self.crop_size)
            post_tensor = self.transform(post_crop)
            return post_tensor, label
            
        except Exception as e:
            print(f"Error processing item {idx}: {e}")
            # Return a placeholder in case of error
            placeholder = torch.zeros(3, self.crop_size, self.crop_size)
            return placeholder, label

    def _center_crop(self, pil_img, minx, miny, maxx, maxy, crop_size):
        width, height = pil_img.size
        bb_width = maxx - minx
        bb_height = maxy - miny
        cx = minx + bb_width / 2.0
        cy = miny + bb_height / 2.0
        
        half = crop_size / 2.0
        left = max(0, min(cx - half, width - crop_size))
        top = max(0, min(cy - half, height - crop_size))
        right = left + crop_size
        bottom = top + crop_size
        return pil_img.crop((left, top, right, bottom))

#-------------------------------------------------------
# TRAINING UTILITIES
#-------------------------------------------------------
def seed_everything(seed=42):
    import random, numpy as np, torch, os
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"Random seed set to {seed}")

def compute_sample_weights(dataset, indices, weight_scale=1.0):
    """Compute sample weights for the given dataset indices"""
    # Extract labels for the given indices
    labels = [dataset.samples[i]["label"] for i in indices]
    counts = Counter(labels)
    total = sum(counts.values())
    num_classes = len(DAMAGE_LABELS)
    
    # Compute inverse frequency weights
    class_weights = {cls: (total / (num_classes * counts[cls]))**weight_scale for cls in counts}
    
    # Create sample weights
    sample_weights = [class_weights[label] for label in labels]
    
    return sample_weights, class_weights

def mixup_data(x, y, alpha=0.2, device='cuda'):
    """Applies mixup augmentation to the data"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Criterion for mixup training"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def plot_learning_curves(epochs, train_losses, val_losses, val_accuracies, per_class_f1, class_names, save_path):
    plt.figure(figsize=(16, 12))
    
    # Plot loss curves
    plt.subplot(2, 2, 1)
    plt.plot(epochs, train_losses, label="Train Loss", marker='o', color='blue')
    plt.plot(epochs, val_losses, label="Val Loss", marker='o', color='red')
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Plot validation accuracy
    plt.subplot(2, 2, 2)
    plt.plot(epochs, val_accuracies, label="Val Accuracy (%)", marker='o', color='green')
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.title("Validation Accuracy")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Plot per-class F1 scores
    plt.subplot(2, 2, 3)
    per_class_f1 = np.array(per_class_f1)  # shape: (num_epochs, num_classes)
    for i, cls_name in enumerate(class_names):
        plt.plot(epochs, per_class_f1[:, i], label=f"{cls_name}", marker='o')
    plt.xlabel("Epoch")
    plt.ylabel("F1 Score")
    plt.title("Per-Class F1 Scores")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Plot class distribution of last batch
    plt.subplot(2, 2, 4)
    plt.bar(class_names, [per_class_f1[-1, i] for i in range(len(class_names))])
    plt.xlabel("Class")
    plt.ylabel("Final F1 Score")
    plt.title("Final F1 Score by Class")
    plt.xticks(rotation=45)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
    print(f"Learning curves plot saved to {save_path}")

class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha  # Weight for each class
        self.reduction = reduction

    def forward(self, inputs, targets):
        # Use nn.functional directly instead of F
        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma * ce_loss)
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss

#-------------------------------------------------------
# MAIN TRAINING FUNCTION
#-------------------------------------------------------
def main():
    start_time = time.time()
    seed_everything(42)
    
    # Hyperparameters & settings
    root_dir = os.path.join(project_root, "data", "xBD")
    batch_size = 32
    lr = 0.0001
    num_epochs = 16
    val_ratio = 0.15
    use_focal_loss = True
    use_mixup = False  # Disable mixup for now
    weight_scale = 0.7
    
    # Determine the next training try number
    output_base_dir = os.path.join(project_root, "output")
    os.makedirs(output_base_dir, exist_ok=True)
    try_number = 1
    while os.path.exists(os.path.join(output_base_dir, f"trainingTry{try_number}")):
        try_number += 1

    # Create output directories
    output_dir = os.path.join(output_base_dir, f"trainingTry{try_number}")
    os.makedirs(output_dir, exist_ok=True)
    model_dir = os.path.join(output_dir, "models")
    os.makedirs(model_dir, exist_ok=True)
    pictures_dir = os.path.join(output_dir, "pictures")
    os.makedirs(pictures_dir, exist_ok=True)
    
    # Save configuration
    config = {
        "try_number": try_number,
        "batch_size": batch_size,
        "learning_rate": lr,
        "num_epochs": num_epochs,
        "val_ratio": val_ratio,
        "use_focal_loss": use_focal_loss,
        "use_mixup": use_mixup,
        "weight_scale": weight_scale,
        "post_disaster_only": True,  # Note that we're only using post-disaster images
    }
    
    with open(os.path.join(output_dir, f"config_try{try_number}.txt"), "w") as f:
        for key, value in config.items():
            f.write(f"{key}: {value}\n")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    print("Initializing XBDPostDisasterDataset for training...")
    full_dataset = XBDPostDisasterDataset(
        root_dir=root_dir,
        crop_size=224,
        max_samples=None,
        flat_structure=False,  # Hierarchical structure: root_dir/disaster/images
        augment=True  # Use augmentation for training
    )
    total_samples = len(full_dataset)
    print(f"Total samples: {total_samples}")

    # Create stratified train-val split directly
    def create_stratified_split(dataset, val_ratio=0.15, seed=42):
        random.seed(seed)
        np.random.seed(seed)
        
        # Group samples by label
        label_to_indices = {}
        for idx, sample in enumerate(dataset.samples):
            label = sample['label']
            if label not in label_to_indices:
                label_to_indices[label] = []
            label_to_indices[label].append(idx)
        
        train_indices = []
        val_indices = []
        
        # Stratified sampling
        for label, indices in label_to_indices.items():
            random.shuffle(indices)
            val_size = int(len(indices) * val_ratio)
            val_indices.extend(indices[:val_size])
            train_indices.extend(indices[val_size:])
        
        random.shuffle(train_indices)
        random.shuffle(val_indices)
        
        return train_indices, val_indices
    
    # Create the split
    train_indices, val_indices = create_stratified_split(full_dataset, val_ratio, seed=42)
    print(f"Training samples: {len(train_indices)}, Validation samples: {len(val_indices)}")

    # Create train and validation datasets
    train_dataset = Subset(full_dataset, train_indices)
    val_dataset = Subset(full_dataset, val_indices)
    
    # Compute sample weights for weighted sampling
    sample_weights, class_weights = compute_sample_weights(full_dataset, train_indices, weight_scale)
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        sampler=sampler, 
        num_workers=4,  # Reduced from 16 to 4 to prevent potential issues
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size,
        shuffle=False,  # No need for sampler with validation set
        num_workers=4,  # Reduced from 16 to 4
        pin_memory=True
    )

    # Initialize model - check if pretrained is a valid parameter
    try:
        model = PostDisasterModel(num_classes=4, pretrained=True, dropout_rate=0.5).to(device)
    except TypeError:
        # If pretrained is not a valid parameter, try without it
        print("Warning: 'pretrained' parameter not supported, using default initialization")
        model = PostDisasterModel(num_classes=4).to(device)
    
    # Define class weights for loss function
    weight_tensor = torch.tensor([class_weights.get(i, 1.0) for i in range(4)], dtype=torch.float).to(device)
    
    # Loss function
    if use_focal_loss:
        criterion = FocalLoss(alpha=weight_tensor, gamma=2.0, reduction='mean')
        print("Using Focal Loss with gamma=2.0")
    else:
        criterion = nn.CrossEntropyLoss(weight=weight_tensor)
        print("Using Weighted CrossEntropyLoss")
    
    # Optimizer with weight decay
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, 
        max_lr=lr,
        epochs=num_epochs,
        steps_per_epoch=len(train_loader),
        pct_start=0.3,
        anneal_strategy='cos'
    )

    # Training metrics tracking
    epochs_list = []
    train_losses = []
    val_losses = []
    val_accuracies = []
    per_class_f1_scores = []
    best_f1_score = 0.0
    
    # Training loop
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        epoch_start_time = time.time()
        
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [TRAIN]", leave=True)
        for post_batch, labels in train_pbar:
            post_batch = post_batch.to(device)
            labels = labels.to(device)
            
            # Apply mixup if enabled
            if use_mixup and np.random.random() < 0.5:
                post_batch, labels_a, labels_b, lam = mixup_data(post_batch, labels, alpha=0.2, device=device)
                mixup_applied = True
            else:
                mixup_applied = False

            optimizer.zero_grad()
            outputs = model(post_batch)
            
            if mixup_applied:
                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)
            else:
                loss = criterion(outputs, labels)
                
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            running_loss += loss.item()
            train_pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validation phase
        model.eval()
        val_loss_epoch = 0.0
        correct = 0
        total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [VAL]", leave=True)
            for post_batch, labels in val_pbar:
                post_batch = post_batch.to(device)
                labels = labels.to(device)
                
                outputs = model(post_batch)
                loss = criterion(outputs, labels)
                val_loss_epoch += loss.item()
                
                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                val_pbar.set_postfix({"loss": f"{loss.item():.4f}"})
                
        val_loss_epoch /= len(val_loader)
        val_losses.append(val_loss_epoch)
        val_acc = 100 * correct / total if total > 0 else 0
        val_accuracies.append(val_acc)
        
        # Calculate F1 scores
        epoch_f1 = f1_score(all_labels, all_preds, average=None, labels=[0, 1, 2, 3])
        per_class_f1_scores.append(epoch_f1)
        
        # Calculate macro F1 for model saving
        macro_f1 = np.mean(epoch_f1)
        epochs_list.append(epoch + 1)
        
        # Calculate elapsed time
        epoch_time = time.time() - epoch_start_time
        
        print(f"Epoch [{epoch+1}/{num_epochs}] "
              f"Train Loss: {avg_train_loss:.4f} | "
              f"Val Loss: {val_loss_epoch:.4f} | "
              f"Val Acc: {val_acc:.2f}% | "
              f"Macro F1: {macro_f1:.4f} | "
              f"Time: {epoch_time:.1f}s")
        
        print(f"Per-class F1 Scores: {', '.join([f'{c}: {f:.4f}' for c, f in zip(DAMAGE_LABELS.keys(), epoch_f1)])}")
        
        # Save the model if it's the best so far
        if macro_f1 > best_f1_score:
            best_f1_score = macro_f1
            best_model_path = os.path.join(model_dir, f"best_model_epoch_{epoch+1}.pt")
            torch.save(model.state_dict(), best_model_path)
            print(f"New best model saved with Macro F1: {best_f1_score:.4f}")

    # Save the final model regardless
    final_save_path = os.path.join(model_dir, f"post_disaster_best.pt")
    torch.save(model.state_dict(), final_save_path)
    print(f"Final model saved to {final_save_path}")
    
    # Create learning curves plot
    plot_save_path = os.path.join(pictures_dir, "learning_curves.png")
    plot_learning_curves(
        epochs_list, 
        train_losses, 
        val_losses, 
        val_accuracies, 
        per_class_f1_scores,
        list(DAMAGE_LABELS.keys()),
        plot_save_path
    )
    
    # Save all training metrics
    metrics = {
        "epochs": epochs_list,
        "train_losses": train_losses,
        "val_losses": val_losses,
        "val_accuracies": val_accuracies,
        "per_class_f1_scores": per_class_f1_scores,
    }
    
    metrics_path = os.path.join(output_dir, f"training_metrics_try{try_number}.txt")
    with open(metrics_path, "w") as f:
        for key, values in metrics.items():
            if key == "per_class_f1_scores":
                f.write(f"{key}:\n")
                for epoch_idx, epoch_scores in enumerate(values):
                    f.write(f"  Epoch {epoch_idx+1}: {epoch_scores}\n")
            else:
                f.write(f"{key}: {values}\n")
    
    total_time = time.time() - start_time
    print(f"Training completed in {total_time/60:.2f} minutes")
    print(f"Best validation Macro F1: {best_f1_score:.4f}")
    print(f"All training artifacts saved to {output_dir}")
    print("Run 'evaluate_baseline.py' to test the model")

if __name__ == "__main__":
    main()


evaluate_baseline:
#!/usr/bin/env python3
import os
import sys
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (confusion_matrix, classification_report,
                             f1_score, precision_score, recall_score,
                             cohen_kappa_score, balanced_accuracy_score,
                             accuracy_score, roc_auc_score, roc_curve, auc)
import time
from datetime import datetime
import json
from PIL import Image
import torchvision.transforms as T
from shapely import wkt
import torchvision.models as models

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.append(project_root)

# Maps the xBD "subtype" to an integer label.
DAMAGE_LABELS = {
    "no-damage": 0,
    "minor-damage": 1,
    "major-damage": 2,
    "destroyed": 3
}

#-------------------------------------------------------
# MODEL DEFINITION (SIMPLIFIED - POST-DISASTER ONLY)
#-------------------------------------------------------
class PostDisasterModel(nn.Module):
    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.5):
        super(PostDisasterModel, self).__init__()
        
        # Use ResNet50 backbone
        try:
            # For torch 1.13+
            base_model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)
        except TypeError:
            # For older torch versions
            base_model = models.resnet50(pretrained=pretrained)
        
        # Extract all layers except the final FC layer
        self.backbone = nn.Sequential(*list(base_model.children())[:-1])
        
        # Get the number of features from the backbone
        self.feature_dim = base_model.fc.in_features  # 2048 for ResNet50
        
        # MLP head with dropout for better generalization
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
        
        # Initialize weights of the classifier
        self._initialize_weights(self.classifier)
        
    def _initialize_weights(self, module):
        for m in module.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                
    def forward(self, post_images):
        # Extract features from post-disaster images
        features = self.backbone(post_images)
        features = torch.flatten(features, 1)
        
        # Classification
        output = self.classifier(features)
        
        return output

#-------------------------------------------------------
# DATASET DEFINITION (MODIFIED - POST-DISASTER ONLY)
#-------------------------------------------------------
class XBDPostDisasterDataset(Dataset):
    """
    Dataset that only uses post-disaster images:
      - Scans the dataset directory.
      - For each post-disaster JSON, enumerates building polygons, computes the bounding box, and crops the post images.
      - Returns (post_patch, label) for each building.
    """
    def __init__(self,
                 root_dir,
                 crop_size=224,
                 use_xy=True,
                 max_samples=None,
                 flat_structure=False,
                 augment=False):
        """
        :param root_dir: Directory where the data is stored.
           - If flat_structure is False: root_dir should contain subfolders for each disaster.
           - If True: root_dir should contain 'images' and 'labels' folders directly.
        :param crop_size: Crop size for post-disaster images.
        :param use_xy: Use 'xy' coordinates if True; else use 'lng_lat'.
        :param max_samples: Optional limit on the number of samples.
        :param flat_structure: Whether the folder structure is flat.
        :param augment: If True, applies data augmentation.
        """
        super().__init__()
        self.root_dir = root_dir
        self.crop_size = crop_size
        self.coord_key = "xy" if use_xy else "lng_lat"
        self.max_samples = max_samples
        self.flat_structure = flat_structure
        self.augment = augment

        # Transform pipeline 
        self.transform = self._get_transform(augment)

        self.samples = self._gather_samples()
        if self.max_samples is not None and len(self.samples) > self.max_samples:
            self.samples = random.sample(self.samples, self.max_samples)

    def _get_transform(self, augment):
        """Create transform pipeline based on whether augmentation is enabled"""
        if augment:
            return T.Compose([
                T.Resize((self.crop_size, self.crop_size)),
                T.RandomHorizontalFlip(),
                T.RandomVerticalFlip(),
                T.RandomRotation(10),
                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
                T.ToTensor(),
                T.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
            ])
        else:
            return T.Compose([
                T.Resize((self.crop_size, self.crop_size)),
                T.ToTensor(),
                T.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
            ])

    def _gather_samples(self):
        samples = []
        
        # Check if using flat structure (test directory)
        images_dir = os.path.join(self.root_dir, "images")
        labels_dir = os.path.join(self.root_dir, "labels")
        
        if os.path.isdir(images_dir) and os.path.isdir(labels_dir):
            # This is a flat structure
            print(f"Detected flat structure at {self.root_dir}")
            label_files = [f for f in os.listdir(labels_dir) if f.endswith("_post_disaster.json")]
            
            for label_file in label_files:
                base_id = label_file.replace("_post_disaster.json", "")
                post_img_name = base_id + "_post_disaster.png"
                post_json_path = os.path.join(labels_dir, label_file)
                post_img_path = os.path.join(images_dir, post_img_name)
                
                if not (os.path.isfile(post_json_path) and os.path.isfile(post_img_path)):
                    continue
                    
                with open(post_json_path, 'r') as f:
                    post_data = json.load(f)
                    
                feats = post_data.get("features", {}).get(self.coord_key, [])
                for feat in feats:
                    damage_type = feat.get("properties", {}).get("subtype", "").lower()
                    if damage_type not in DAMAGE_LABELS:
                        continue
                        
                    label = DAMAGE_LABELS[damage_type]
                    wkt_str = feat.get("wkt", None)
                    if wkt_str is None:
                        continue
                        
                    polygon = wkt.loads(wkt_str)
                    minx, miny, maxx, maxy = polygon.bounds
                    samples.append({
                        "post_img": post_img_path,
                        "bbox": (minx, miny, maxx, maxy),
                        "label": label
                    })
        else:
            # Hierarchical structure: root_dir contains subfolders per disaster.
            try:
                disasters = [d for d in os.listdir(self.root_dir)
                            if os.path.isdir(os.path.join(self.root_dir, d))
                            and d.lower() != "spacenet_gt"]
                
                print(f"Found {len(disasters)} disaster folders")
                
                for disaster in disasters:
                    disaster_dir = os.path.join(self.root_dir, disaster)
                    images_dir = os.path.join(disaster_dir, "images")
                    labels_dir = os.path.join(disaster_dir, "labels")
                    
                    if not (os.path.isdir(images_dir) and os.path.isdir(labels_dir)):
                        print(f"Warning: Missing images or labels directory for disaster: {disaster}")
                        continue
                        
                    label_files = [f for f in os.listdir(labels_dir) if f.endswith("_post_disaster.json")]
                    print(f"Disaster {disaster}: Found {len(label_files)} label files")
                    
                    for label_file in label_files:
                        base_id = label_file.replace("_post_disaster.json", "")
                        post_img_name = base_id + "_post_disaster.png"
                        post_json_path = os.path.join(labels_dir, label_file)
                        post_img_path = os.path.join(images_dir, post_img_name)
                        
                        if not (os.path.isfile(post_json_path) and os.path.isfile(post_img_path)):
                            continue
                            
                        with open(post_json_path, 'r') as f:
                            post_data = json.load(f)
                            
                        feats = post_data.get("features", {}).get(self.coord_key, [])
                        for feat in feats:
                            damage_type = feat.get("properties", {}).get("subtype", "").lower()
                            if damage_type not in DAMAGE_LABELS:
                                continue
                                
                            label = DAMAGE_LABELS[damage_type]
                            wkt_str = feat.get("wkt", None)
                            if wkt_str is None:
                                continue
                                
                            polygon = wkt.loads(wkt_str)
                            minx, miny, maxx, maxy = polygon.bounds
                            samples.append({
                                "post_img": post_img_path,
                                "bbox": (minx, miny, maxx, maxy),
                                "label": label,
                                "disaster": disaster  # Track which disaster this is from
                            })
            except Exception as e:
                print(f"Error gathering samples: {e}")
                
        print(f"Total gathered samples: {len(samples)}")
        # Check class distribution
        labels = [s["label"] for s in samples]
        unique_labels, counts = np.unique(labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = [name for name, idx in DAMAGE_LABELS.items() if idx == label][0]
            print(f"  {class_name}: {count} samples ({count/len(samples)*100:.2f}%)")
            
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        post_path = item["post_img"]
        (minx, miny, maxx, maxy) = item["bbox"]
        label = item["label"]

        try:
            post_img = Image.open(post_path).convert("RGB")
            post_crop = self._center_crop(post_img, minx, miny, maxx, maxy, self.crop_size)
            post_tensor = self.transform(post_crop)

            return post_tensor, label
            
        except Exception as e:
            print(f"Error processing item {idx}: {e}")
            # Return a placeholder in case of error
            placeholder_post = torch.zeros(3, self.crop_size, self.crop_size)
            return placeholder_post, label

    def _center_crop(self, pil_img, minx, miny, maxx, maxy, crop_size):
        width, height = pil_img.size
        bb_width = maxx - minx
        bb_height = maxy - miny
        cx = minx + bb_width / 2.0
        cy = miny + bb_height / 2.0
        
        half = crop_size / 2.0
        left = max(0, min(cx - half, width - crop_size))
        top = max(0, min(cy - half, height - crop_size))
        right = left + crop_size
        bottom = top + crop_size
        return pil_img.crop((left, top, right, bottom))

#-------------------------------------------------------
# EVALUATION UTILITIES
#-------------------------------------------------------
def seed_everything(seed=42):
    """Set random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"Random seed set to {seed}")

def test_time_augmentation(model, post_img, device, num_augments=5):
    """Apply test-time augmentation by running multiple forward passes with different augmentations."""
    model.eval()
    batch_size = post_img.size(0)
    
    # Initialize with original predictions
    with torch.no_grad():
        outputs_original = model(post_img)
    
    all_outputs = [outputs_original]
    
    # Horizontal flip augmentation
    with torch.no_grad():
        post_hflip = torch.flip(post_img, dims=[3])
        outputs_hflip = model(post_hflip)
        all_outputs.append(outputs_hflip)
    
    # Vertical flip augmentation
    with torch.no_grad():
        post_vflip = torch.flip(post_img, dims=[2])
        outputs_vflip = model(post_vflip)
        all_outputs.append(outputs_vflip)
    
    # Both horizontal and vertical flip
    with torch.no_grad():
        post_hvflip = torch.flip(post_img, dims=[2, 3])
        outputs_hvflip = model(post_hvflip)
        all_outputs.append(outputs_hvflip)
    
    # Center crop augmentation (90% of image)
    if num_augments > 4:
        with torch.no_grad():
            h, w = post_img.shape[2:]
            ch, cw = int(h * 0.1), int(w * 0.1)
            post_crop = post_img[:, :, ch:h-ch, cw:w-cw]
            post_crop = F.interpolate(post_crop, size=(h, w), mode='bilinear', align_corners=False)
            outputs_crop = model(post_crop)
            all_outputs.append(outputs_crop)
    
    # Average the predictions
    outputs = torch.stack(all_outputs).mean(dim=0)
    return outputs

def evaluate_model(model, dataloader, device, use_tta=False):
    """Evaluate model with optional test-time augmentation."""
    model.eval()
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    all_probs = []  # For ROC curves
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Evaluating", leave=True)
        for post_imgs, labels in pbar:
            post_imgs = post_imgs.to(device)
            labels = labels.to(device)
            
            # Either use test-time augmentation or standard forward pass
            if use_tta:
                outputs = test_time_augmentation(model, post_imgs, device)
            else:
                outputs = model(post_imgs)
            
            # Get predictions and probabilities
            probabilities = F.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)
            
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probabilities.cpu().numpy())
            
            # Update progress bar
            pbar.set_postfix({"Acc": f"{100 * correct / total:.2f}%"})
    
    accuracy = 100 * correct / total if total > 0 else 0
    return accuracy, np.array(all_preds), np.array(all_labels), np.array(all_probs)

def plot_confusion_matrix(cm, target_names, save_path, normalize=False, title=None):
    """
    Generate and save a confusion matrix plot.
    
    Parameters:
    - cm: Confusion matrix from sklearn
    - target_names: List of class names
    - save_path: Path to save the plot
    - normalize: Boolean, whether to normalize values
    - title: Title for the plot
    """
    if normalize:
        cm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-6)
        fmt = '.2f'
    else:
        fmt = 'd'
    
    plt.figure(figsize=(10, 8))
    sns.set(font_scale=1.2)
    sns.heatmap(cm, annot=True, fmt=fmt, cmap="Blues",
                xticklabels=target_names, yticklabels=target_names)
    
    plt.ylabel('True Label', fontsize=14)
    plt.xlabel('Predicted Label', fontsize=14)
    
    if title:
        plt.title(title, fontsize=16)
    else:
        plt.title("Confusion Matrix", fontsize=16)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

def plot_per_class_metrics(metrics, metric_name, target_names, save_path):
    """Generate and save per-class metrics bar chart."""
    plt.figure(figsize=(12, 6))
    x = np.arange(len(target_names))
    
    bar_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    bars = plt.bar(x, metrics, color=bar_colors, width=0.6)
    
    # Add value labels on top of each bar
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', fontsize=10)
    
    plt.axhline(y=np.mean(metrics), color='r', linestyle='--', label=f'Mean: {np.mean(metrics):.3f}')
    
    plt.xticks(x, target_names, rotation=30)
    plt.ylim(0, min(1, max(metrics) + 0.15))
    
    plt.xlabel("Classes", fontsize=12)
    plt.ylabel(metric_name, fontsize=12)
    plt.title(f"Per-Class {metric_name}", fontsize=14)
    
    plt.legend()
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

def plot_roc_curves(all_probs, all_labels, target_names, save_path):
    """Generate and save ROC curves for each class."""
    plt.figure(figsize=(12, 8))
    
    # One-hot encode the labels for multi-class ROC
    n_classes = len(target_names)
    y_true_onehot = np.eye(n_classes)[all_labels]
    
    # Colors for each class
    colors = ['blue', 'orange', 'green', 'red']
    
    # Calculate ROC curve and ROC area for each class
    for i, color, target_name in zip(range(n_classes), colors, target_names):
        fpr, tpr, _ = roc_curve(y_true_onehot[:, i], all_probs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=color, lw=2,
                 label=f'{target_name} (AUC = {roc_auc:.3f})')
    
    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=14)
    plt.legend(loc="lower right")
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

def analyze_errors(all_preds, all_labels, all_probs, target_names, save_path):
    """Analyze prediction errors to find common patterns."""
    # Create a figure for error analysis
    plt.figure(figsize=(12, 8))
    
    # Get indices of errors
    error_idx = np.where(all_preds != all_labels)[0]
    correct_idx = np.where(all_preds == all_labels)[0]
    
    # Calculate error rates for each true class
    error_rates = []
    error_types = []
    
    for true_class in range(len(target_names)):
        # Get all samples of this true class
        class_samples = np.where(all_labels == true_class)[0]
        # Find errors among these samples
        class_errors = np.intersect1d(class_samples, error_idx)
        # Calculate error rate
        error_rate = len(class_errors) / len(class_samples) if len(class_samples) > 0 else 0
        error_rates.append(error_rate)
        
        # Find common misclassifications
        if len(class_errors) > 0:
            error_preds = all_preds[class_errors]
            unique_preds, counts = np.unique(error_preds, return_counts=True)
            most_common_idx = np.argmax(counts)
            most_common_class = unique_preds[most_common_idx]
            error_types.append(f"{target_names[true_class]} → {target_names[most_common_class]}")
        else:
            error_types.append("No errors")
    
    # Plot error rates
    plt.subplot(2, 1, 1)
    bars = plt.bar(target_names, error_rates, color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'])
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{height:.2f}', ha='center', fontsize=10)
    
    plt.title("Error Rate by True Class", fontsize=14)
    plt.ylabel("Error Rate", fontsize=12)
    plt.ylim(0, max(error_rates) + 0.1)
    
    # Analyze confidence of correct vs. incorrect predictions
    plt.subplot(2, 1, 2)
    
    # Get confidence (probability of predicted class)
    confidences = np.array([all_probs[i, pred] for i, pred in enumerate(all_preds)])
    
    # Split by correct/incorrect
    correct_conf = confidences[correct_idx]
    error_conf = confidences[error_idx]
    
    # Create histogram
    plt.hist(correct_conf, bins=20, alpha=0.5, label=f'Correct (n={len(correct_conf)})', color='green')
    plt.hist(error_conf, bins=20, alpha=0.5, label=f'Incorrect (n={len(error_conf)})', color='red')
    
    plt.xlabel("Prediction Confidence", fontsize=12)
    plt.ylabel("Count", fontsize=12)
    plt.title("Confidence Distribution for Correct vs. Incorrect Predictions", fontsize=14)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    
    # Print error analysis summary
    print("\nError Analysis:")
    print("  Most common misclassification patterns:")
    for i, error in enumerate(error_types):
        print(f"  - {error} (Error rate: {error_rates[i]:.2f})")

def main():
    # Set random seed for reproducibility
    seed_everything(42)
    
    # Configuration
    use_tta = True  # Use test-time augmentation
    checkpoint_name = "output/trainingTry9/models/post_disaster_best.pt"  # Evaluate the post-only model

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # Create timestamp for saving results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Test data is stored in a hierarchical structure under data/test
    test_data_dir = os.path.join(project_root, "data", "test")
    print(f"Loading evaluation data from: {test_data_dir}")

    # Create test dataset without augmentation
    test_dataset = XBDPostDisasterDataset(
        root_dir=test_data_dir,
        crop_size=224,
        use_xy=True,
        max_samples=None,  # No limit on samples for evaluation
        flat_structure=True,  # Test data is flat structure with images/ and labels/ folders
        augment=False         # No augmentation during evaluation
    )
    print(f"Total evaluation samples: {len(test_dataset)}")

    # Create test dataloader with optimizations
    test_loader = DataLoader(
        test_dataset, 
        batch_size=64,  # Increased batch size for faster evaluation
        shuffle=False, 
        num_workers=4,
        pin_memory=True
    )

    # Initialize model
    model = PostDisasterModel(num_classes=4).to(device)
    
    # Load checkpoint
    checkpoint_path = os.path.join(project_root, checkpoint_name)
    if os.path.isfile(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint, strict=False)
        print(f"Loaded model weights from {checkpoint_path}")
    else:
        print(f"Model checkpoint not found at {checkpoint_path}")
        return

    # Evaluate model
    print(f"Starting evaluation{' with test-time augmentation' if use_tta else ''}...")
    start_time = time.time()
    accuracy, preds, labels, probs = evaluate_model(model, test_loader, device, use_tta=use_tta)
    eval_time = time.time() - start_time
    
    print(f"\nEvaluation completed in {eval_time:.2f} seconds")
    print(f"Final Evaluation Accuracy: {accuracy:.2f}%\n")

    # Compute detailed metrics
    target_names = list(DAMAGE_LABELS.keys())
    
    # Check if there are any predictions
    if len(labels) == 0:
        print("No samples were evaluated. Exiting...")
        return

    # Create confusion matrix
    conf_matrix = confusion_matrix(labels, preds)
    print("Confusion Matrix:")
    print(conf_matrix)

    # Create classification report with proper handling of empty arrays
    if len(labels) > 0 and len(np.unique(labels)) > 0:
        report = classification_report(labels, preds, target_names=target_names, digits=4)
        print("\nClassification Report:")
        print(report)

        # Calculate additional metrics
        macro_f1 = f1_score(labels, preds, average='macro')
        weighted_f1 = f1_score(labels, preds, average='weighted')
        macro_precision = precision_score(labels, preds, average='macro')
        weighted_precision = precision_score(labels, preds, average='weighted')
        macro_recall = recall_score(labels, preds, average='macro')
        weighted_recall = recall_score(labels, preds, average='weighted')
        kappa = cohen_kappa_score(labels, preds)
        bal_accuracy = balanced_accuracy_score(labels, preds)
    else:
        print("No valid labels or predictions to calculate metrics.")
        macro_f1 = weighted_f1 = macro_precision = weighted_precision = macro_recall = weighted_recall = kappa = bal_accuracy = 0.0

    print("Additional Metrics:")
    print(f"Macro F1 Score: {macro_f1:.4f}")
    print(f"Weighted F1 Score: {weighted_f1:.4f}")
    print(f"Macro Precision: {macro_precision:.4f}")
    print(f"Weighted Precision: {weighted_precision:.4f}")
    print(f"Macro Recall: {macro_recall:.4f}")
    print(f"Weighted Recall: {weighted_recall:.4f}")
    print(f"Cohen's Kappa: {kappa:.4f}")
    print(f"Balanced Accuracy: {bal_accuracy:.4f}")

    # Save detailed results to a text file
    results_dir = os.path.join(project_root, "evaluation_results")
    os.makedirs(results_dir, exist_ok=True)
    
    results_path = os.path.join(results_dir, f"post_only_evaluation_{timestamp}.txt")
    with open(results_path, 'w') as f:
        f.write(f"Post-Disaster Only Evaluation Results for {checkpoint_name}\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Test-Time Augmentation: {use_tta}\n\n")
        
        if len(labels) > 0:
            f.write("Confusion Matrix:\n")
            f.write(str(conf_matrix) + "\n\n")
            
            if len(np.unique(labels)) > 0:
                f.write("Classification Report:\n")
                f.write(report + "\n\n")
        else:
            f.write("No samples were evaluated.\n\n")
        
        f.write("Additional Metrics:\n")
        f.write(f"Accuracy: {accuracy:.4f}%\n")
        f.write(f"Macro F1 Score: {macro_f1:.4f}\n")
        f.write(f"Weighted F1 Score: {weighted_f1:.4f}\n")
        f.write(f"Macro Precision: {macro_precision:.4f}\n")
        f.write(f"Weighted Precision: {weighted_precision:.4f}\n")
        f.write(f"Macro Recall: {macro_recall:.4f}\n")
        f.write(f"Weighted Recall: {weighted_recall:.4f}\n")
        f.write(f"Cohen's Kappa: {kappa:.4f}\n")
        f.write(f"Balanced Accuracy: {bal_accuracy:.4f}\n")
    
    print(f"Detailed evaluation results saved to {results_path}")

    # Create directory for saving plots
    pictures_dir = os.path.join(project_root, "pictures", f"post_only_evaluation_{timestamp}")
    os.makedirs(pictures_dir, exist_ok=True)
    print(f"Saving visualizations to {pictures_dir}")

    # Skip plotting if no samples were evaluated
    if len(labels) == 0 or len(np.unique(labels)) == 0:
        print("No samples or unique labels to generate visualizations.")
        return

    # Plot and save confusion matrix
    cm_save_path = os.path.join(pictures_dir, "confusion_matrix.png")
    plot_confusion_matrix(conf_matrix, target_names, cm_save_path, 
                         title=f"Confusion Matrix (Accuracy: {accuracy:.2f}%)")
    
    cm_norm_save_path = os.path.join(pictures_dir, "confusion_matrix_normalized.png")
    plot_confusion_matrix(conf_matrix, target_names, cm_norm_save_path, normalize=True,
                         title="Normalized Confusion Matrix")

    # Plot per-class metrics
    try:
        per_class_f1 = f1_score(labels, preds, average=None, labels=range(len(target_names)))
        per_class_precision = precision_score(labels, preds, average=None, labels=range(len(target_names)))
        per_class_recall = recall_score(labels, preds, average=None, labels=range(len(target_names)))
        
        f1_save_path = os.path.join(pictures_dir, "per_class_f1.png")
        plot_per_class_metrics(per_class_f1, "F1 Score", target_names, f1_save_path)
        
        precision_save_path = os.path.join(pictures_dir, "per_class_precision.png")
        plot_per_class_metrics(per_class_precision, "Precision", target_names, precision_save_path)
        
        recall_save_path = os.path.join(pictures_dir, "per_class_recall.png")
        plot_per_class_metrics(per_class_recall, "Recall", target_names, recall_save_path)
        
        # Plot ROC curves for multi-class classification
        roc_save_path = os.path.join(pictures_dir, "roc_curves.png")
        plot_roc_curves(probs, labels, target_names, roc_save_path)
        
        # Analyze errors
        error_save_path = os.path.join(pictures_dir, "error_analysis.png")
        analyze_errors(preds, labels, probs, target_names, error_save_path)
    except Exception as e:
        print(f"Error generating metric plots: {e}")
    
    # Randomly sample predictions to show variety if we have predictions
    if len(preds) > 0:
        num_samples = min(15, len(preds))
        sample_indices = random.sample(range(len(preds)), num_samples)
        
        print("\nSample Predictions (randomly selected):")
        print(f"{'Index':<8} {'True Label':<15} {'Predicted':<15} {'Confidence':<10} {'Correct':<8}")
        print("-" * 60)
        
        for i in sample_indices:
            pred_class = target_names[preds[i]]
            true_class = target_names[labels[i]]
            confidence = probs[i, preds[i]]
            correct = "✓" if preds[i] == labels[i] else "✗"
            print(f"{i:<8} {true_class:<15} {pred_class:<15} {confidence:.4f}      {correct}")

if __name__ == "__main__":
    main()


results:
(myenv) pablos@debian:~/Documents/uc3m/Incase$ /home/pablos/Documents/uc3m/Incase/myenv/bin/python /home/pablos/Documents/uc3m/Incase/scripts/training/evaluate_baseline.py
Random seed set to 42
Using device: cuda
Loading evaluation data from: /home/pablos/Documents/uc3m/Incase/data/test
Detected flat structure at /home/pablos/Documents/uc3m/Incase/data/test
Total gathered samples: 53850
Class distribution:
  no-damage: 41427 samples (76.93%)
  minor-damage: 4798 samples (8.91%)
  major-damage: 3850 samples (7.15%)
  destroyed: 3775 samples (7.01%)
Total evaluation samples: 53850
Loaded model weights from /home/pablos/Documents/uc3m/Incase/output/trainingTry14/models/post_disaster_best.pt
Starting evaluation with test-time augmentation...
Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 842/842 [07:45<00:00,  1.81it/s, Acc=53.55%]

Evaluation completed in 465.36 seconds
Final Evaluation Accuracy: 53.55%

Confusion Matrix:
[[19015 13401  4342  4669]
 [   69  3616   799   314]
 [   41   357  2984   468]
 [   84   153   319  3219]]

Classification Report:
              precision    recall  f1-score   support

   no-damage     0.9899    0.4590    0.6272     41427
minor-damage     0.2063    0.7536    0.3239      4798
major-damage     0.3534    0.7751    0.4854      3850
   destroyed     0.3713    0.8527    0.5173      3775

    accuracy                         0.5355     53850
   macro avg     0.4802    0.7101    0.4885     53850
weighted avg     0.8312    0.5355    0.5823     53850

Additional Metrics:
Macro F1 Score: 0.4885
Weighted F1 Score: 0.5823
Macro Precision: 0.4802
Weighted Precision: 0.8312
Macro Recall: 0.7101
Weighted Recall: 0.5355
Cohen's Kappa: 0.3108
Balanced Accuracy: 0.7101
Detailed evaluation results saved to /home/pablos/Documents/uc3m/Incase/evaluation_results/improved_post_only_evaluation_20250304_102807.txt
Saving visualizations to /home/pablos/Documents/uc3m/Incase/pictures/improved_evaluation_20250304_102807

Error Analysis:
  Most common misclassification patterns:
  - no-damage → minor-damage (Error rate: 0.54)
  - minor-damage → major-damage (Error rate: 0.25)
  - major-damage → destroyed (Error rate: 0.22)
  - destroyed → major-damage (Error rate: 0.15)

Sample Predictions (randomly selected):
Index    True Label      Predicted       Confidence Correct 
------------------------------------------------------------
41905    no-damage       minor-damage    0.5004      ✗
7296     no-damage       destroyed       0.3800      ✗
1639     minor-damage    major-damage    0.5482      ✗
48598    no-damage       no-damage       0.3059      ✓
18024    no-damage       major-damage    0.3564      ✗
16049    no-damage       no-damage       0.8948      ✓
14628    no-damage       destroyed       0.4976      ✗
9144     no-damage       no-damage       0.6907      ✓
48265    no-damage       no-damage       0.3448      ✓
6717     no-damage       minor-damage    0.3699      ✗
44348    no-damage       minor-damage    0.5257      ✗
48540    no-damage       minor-damage    0.4503      ✗
35741    major-damage    major-damage    0.8058      ✓
5697     no-damage       minor-damage    0.4767      ✗
38698    no-damage       no-damage       0.4764      ✓

Evaluation complete!
Accuracy: 53.55%
Macro F1: 0.4885
Weighted F1: 0.5823