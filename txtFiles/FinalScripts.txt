train_baseline:
#!/usr/bin/env python3
# xBD Building Damage Assessment Model
# This script implements a deep learning pipeline for detecting building damage
# from satellite imagery before and after natural disasters
import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, Subset
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import f1_score
import random
from datetime import datetime
import time
import torch.nn.functional as F
import json
from PIL import Image
import torchvision.transforms as T
from shapely import wkt
import shutil  # Added for file operations

# Constants and definitions
# Dictionary mapping damage category labels to numerical class indices
DAMAGE_LABELS = {
    "no-damage": 0,
    "minor-damage": 1,
    "major-damage": 2,
    "destroyed": 3
}

# Dataset class
class XBDPatchDataset(Dataset):
    """
    On-the-fly dataset that:
      - Scans the dataset directory.
      - In hierarchical mode (flat_structure=False): expects subfolders per disaster (each with an 'images' and 'labels' folder).
      - In flat mode (flat_structure=True): expects root_dir to contain 'images' and 'labels' folders directly.
      - For each post-disaster JSON, enumerates building polygons, computes the bounding box, and center-crops the pre & post images.
      - Returns (pre_patch, post_patch, label) for each building.
    """
    def __init__(self,
                 root_dir,
                 pre_crop_size=128,
                 post_crop_size=224,
                 use_xy=True,
                 max_samples=None,
                 flat_structure=False,
                 augment=False):
        """
        :param root_dir: Directory where the data is stored.
           - If flat_structure is False: root_dir should contain subfolders for each disaster.
           - If True: root_dir should contain 'images' and 'labels' folders directly.
        :param pre_crop_size: Crop size for pre-disaster images.
        :param post_crop_size: Crop size for post-disaster images.
        :param use_xy: Use 'xy' coordinates if True; else use 'lng_lat'.
        :param max_samples: Optional limit on the number of samples.
        :param flat_structure: Whether the folder structure is flat.
        :param augment: If True, applies data augmentation (random horizontal flip and random rotation).
        """
        super().__init__()
        self.root_dir = root_dir
        self.pre_crop_size = pre_crop_size
        self.post_crop_size = post_crop_size
        self.coord_key = "xy" if use_xy else "lng_lat"
        self.max_samples = max_samples
        self.flat_structure = flat_structure
        self.augment = augment

        # Basic transforms - keeping it simple for compatibility
        # Define transforms for pre-disaster images (resize to specified size and normalize)
        self.pre_transform = T.Compose([
            T.Resize((pre_crop_size, pre_crop_size)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
        ])
        
        # Define transforms for post-disaster images (resize to specified size and normalize)
        self.post_transform = T.Compose([
            T.Resize((post_crop_size, post_crop_size)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
        ])

        # Gather all samples from the dataset directory structure
        self.samples = self._gather_samples()
        
        # Limit the number of samples if specified
        if self.max_samples is not None and len(self.samples) > self.max_samples:
            self.samples = random.sample(self.samples, self.max_samples)

    def _gather_samples(self):
        """
        Parse the dataset directory to find all valid sample pairs (pre/post disaster images with labels).
        Supports both flat and hierarchical directory structures.
        Returns a list of dictionaries with paths to images and corresponding labels.
        """
        samples = []
        # First try to check if "test" directory has a flat structure
        images_dir = os.path.join(self.root_dir, "images")
        labels_dir = os.path.join(self.root_dir, "labels")
        
        if os.path.isdir(images_dir) and os.path.isdir(labels_dir):
            # This looks like a flat structure
            print(f"Detected flat structure at {self.root_dir}")
            label_files = [f for f in os.listdir(labels_dir) if f.endswith("_post_disaster.json")]
            
            for label_file in label_files:
                # Extract base ID from filename (removing the _post_disaster.json suffix)
                base_id = label_file.replace("_post_disaster.json", "")
                post_img_name = base_id + "_post_disaster.png"
                pre_img_name = base_id + "_pre_disaster.png"
                post_json_path = os.path.join(labels_dir, label_file)
                post_img_path = os.path.join(images_dir, post_img_name)
                pre_json_path = os.path.join(labels_dir, base_id + "_pre_disaster.json")
                pre_img_path = os.path.join(images_dir, pre_img_name)
                
                # Skip if any of the required files don't exist
                if not (os.path.isfile(post_json_path) and os.path.isfile(post_img_path)
                        and os.path.isfile(pre_json_path) and os.path.isfile(pre_img_path)):
                    continue
                    
                # Load the post-disaster JSON data which contains damage labels
                with open(post_json_path, 'r') as f:
                    post_data = json.load(f)
                    
                # Extract feature information from the JSON data
                feats = post_data.get("features", {}).get(self.coord_key, [])
                for feat in feats:
                    # Get the damage type (subtype) and convert to lowercase
                    damage_type = feat.get("properties", {}).get("subtype", "").lower()
                    if damage_type not in DAMAGE_LABELS:
                        continue
                        
                    # Convert damage type to numerical label
                    label = DAMAGE_LABELS[damage_type]
                    wkt_str = feat.get("wkt", None)
                    if wkt_str is None:
                        continue
                        
                    # Parse the WKT string to get a polygon object and extract its bounding box
                    polygon = wkt.loads(wkt_str)
                    minx, miny, maxx, maxy = polygon.bounds
                    samples.append({
                        "pre_img": pre_img_path,
                        "post_img": post_img_path,
                        "bbox": (minx, miny, maxx, maxy),
                        "label": label
                    })
        else:
            # Hierarchical structure: root_dir contains subfolders per disaster.
            try:
                # List all directories in the root path (excluding 'spacenet_gt' folder)
                disasters = [d for d in os.listdir(self.root_dir)
                            if os.path.isdir(os.path.join(self.root_dir, d))
                            and d.lower() != "spacenet_gt"]
                
                print(f"Found {len(disasters)} disaster folders")
                
                # Process each disaster folder
                for disaster in disasters:
                    disaster_dir = os.path.join(self.root_dir, disaster)
                    images_dir = os.path.join(disaster_dir, "images")
                    labels_dir = os.path.join(disaster_dir, "labels")
                    
                    # Skip if images or labels directory is missing
                    if not (os.path.isdir(images_dir) and os.path.isdir(labels_dir)):
                        print(f"Warning: Missing images or labels directory for disaster: {disaster}")
                        continue
                        
                    # Find all post-disaster JSON files (which contain damage labels)
                    label_files = [f for f in os.listdir(labels_dir) if f.endswith("_post_disaster.json")]
                    print(f"Disaster {disaster}: Found {len(label_files)} label files")
                    
                    # Process each label file
                    for label_file in label_files:
                        base_id = label_file.replace("_post_disaster.json", "")
                        post_img_name = base_id + "_post_disaster.png"
                        pre_img_name = base_id + "_pre_disaster.png"
                        post_json_path = os.path.join(labels_dir, label_file)
                        post_img_path = os.path.join(images_dir, post_img_name)
                        pre_json_path = os.path.join(labels_dir, base_id + "_pre_disaster.json")
                        pre_img_path = os.path.join(images_dir, pre_img_name)
                        
                        # Skip if any required files are missing
                        if not (os.path.isfile(post_json_path) and os.path.isfile(post_img_path)
                                and os.path.isfile(pre_json_path) and os.path.isfile(pre_img_path)):
                            continue
                            
                        # Load the post-disaster JSON data
                        with open(post_json_path, 'r') as f:
                            post_data = json.load(f)
                            
                        # Process each feature in the JSON data
                        feats = post_data.get("features", {}).get(self.coord_key, [])
                        for feat in feats:
                            damage_type = feat.get("properties", {}).get("subtype", "").lower()
                            if damage_type not in DAMAGE_LABELS:
                                continue
                                
                            label = DAMAGE_LABELS[damage_type]
                            wkt_str = feat.get("wkt", None)
                            if wkt_str is None:
                                continue
                                
                            # Parse the WKT string to get the polygon and extract its bounding box
                            polygon = wkt.loads(wkt_str)
                            minx, miny, maxx, maxy = polygon.bounds
                            samples.append({
                                "pre_img": pre_img_path,
                                "post_img": post_img_path,
                                "bbox": (minx, miny, maxx, maxy),
                                "label": label,
                                "disaster": disaster  # Track which disaster this is from
                            })
            except Exception as e:
                print(f"Error gathering samples: {e}")
                
        print(f"Total gathered samples: {len(samples)}")
        # Check class distribution
        labels = [s["label"] for s in samples]
        unique_labels, counts = np.unique(labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = [name for name, idx in DAMAGE_LABELS.items() if idx == label][0]
            print(f"  {class_name}: {count} samples ({count/len(samples)*100:.2f}%)")
            
        return samples

    def __len__(self):
        """Return the number of samples in the dataset."""
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Get a single sample from the dataset.
        Returns pre-disaster image, post-disaster image, and damage label.
        """
        item = self.samples[idx]
        pre_path = item["pre_img"]
        post_path = item["post_img"]
        (minx, miny, maxx, maxy) = item["bbox"]
        label = item["label"]

        try:
            # Load the pre and post disaster images
            pre_img = Image.open(pre_path).convert("RGB")
            post_img = Image.open(post_path).convert("RGB")

            # Crop the images around the building bounding box
            pre_crop = self._center_crop(pre_img, minx, miny, maxx, maxy, self.pre_crop_size)
            post_crop = self._center_crop(post_img, minx, miny, maxx, maxy, self.post_crop_size)

            # Use consistent random state for both images if augmentation is enabled
            # This ensures the same random transformations are applied to both pre and post images
            if self.augment:
                seed = np.random.randint(2147483647)
                random.seed(seed)
                torch.manual_seed(seed)
                
            # Apply transformations to pre-disaster image
            pre_tensor = self.pre_transform(pre_crop)
            
            if self.augment:
                # Reset the seed for the second transform to ensure same transformation
                random.seed(seed)
                torch.manual_seed(seed)
                
            # Apply transformations to post-disaster image
            post_tensor = self.post_transform(post_crop)

            return pre_tensor, post_tensor, label
            
        except Exception as e:
            print(f"Error processing item {idx}: {e}")
            # Return a placeholder in case of error (zero tensors and the original label)
            placeholder_pre = torch.zeros(3, self.pre_crop_size, self.pre_crop_size)
            placeholder_post = torch.zeros(3, self.post_crop_size, self.post_crop_size)
            return placeholder_pre, placeholder_post, label

    def _center_crop(self, pil_img, minx, miny, maxx, maxy, crop_size):
        """
        Crop the image around the center of the given bounding box with specified crop size.
        Ensures the crop is within the image boundaries.
        """
        width, height = pil_img.size
        bb_width = maxx - minx
        bb_height = maxy - miny
        # Find the center of the bounding box
        cx = minx + bb_width / 2.0
        cy = miny + bb_height / 2.0
        
        # Calculate crop boundaries ensuring they're within image dimensions
        half = crop_size / 2.0
        left = max(0, min(cx - half, width - crop_size))
        top = max(0, min(cy - half, height - crop_size))
        right = left + crop_size
        bottom = top + crop_size
        return pil_img.crop((left, top, right, bottom))

# Model definition
class AttentionFusion(nn.Module):
    """
    Attention-based feature fusion module that combines features from pre- and post-disaster images.
    Uses a learned attention mechanism to focus on the most relevant features for damage assessment.
    """
    def __init__(self, in_channels):
        super(AttentionFusion, self).__init__()
        # Attention mechanism implemented as a small convolutional network
        self.attention = nn.Sequential(
            nn.Conv2d(in_channels * 2, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, 1, kernel_size=1),
            nn.Sigmoid()  # Outputs values between 0-1 for attention weights
        )
        
    def forward(self, pre_feat, post_feat):
        """
        Forward pass of the attention fusion module.
        Args:
            pre_feat: Features from pre-disaster image
            post_feat: Features from post-disaster image
        Returns:
            Fused feature map combining information from both inputs
        """
        # Make sure the spatial dimensions match before concatenating
        if pre_feat.shape[2:] != post_feat.shape[2:]:
            # Resize post_feat to match pre_feat's spatial dimensions
            post_feat = F.interpolate(post_feat, size=pre_feat.shape[2:], 
                                      mode='bilinear', align_corners=False)
            
        # Concatenate features along channel dimension
        concat_feat = torch.cat([pre_feat, post_feat], dim=1)
        # Generate attention weights
        attn_weights = self.attention(concat_feat)
        # Apply attention to post features
        weighted_post = post_feat * attn_weights
        # Combine pre and weighted post features
        fused_feat = pre_feat + weighted_post
        return fused_feat

class BaselineModel(nn.Module):
    """
    Siamese-style network that processes pre- and post-disaster images separately,
    then fuses their features using an attention mechanism.
    Uses ResNet50 as the backbone for feature extraction.
    """
    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.5):
        super(BaselineModel, self).__init__()
        
        # Use a more powerful backbone (ResNet50)
        try:
            # For torch 1.13+
            base_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights='IMAGENET1K_V2' if pretrained else None)
        except:
            try:
                # For torch 1.13+
                import torchvision.models as models
                base_model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)
            except TypeError:
                # For older torch versions
                import torchvision.models as models
                base_model = models.resnet50(pretrained=pretrained)
        
        # Pre-disaster branch - uses layers up to the final FC layer of ResNet50
        self.pre_branch = nn.Sequential(*list(base_model.children())[:-2])
        
        # Post-disaster branch (same architecture but separate weights)
        try:
            # For torch 1.13+
            post_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights='IMAGENET1K_V2' if pretrained else None)
        except:
            try:
                # For torch 1.13+
                import torchvision.models as models
                post_model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)
            except TypeError:
                # For older torch versions
                import torchvision.models as models
                post_model = models.resnet50(pretrained=pretrained)
        
        self.post_branch = nn.Sequential(*list(post_model.children())[:-2])
        
        # Get the number of features from the backbone
        self.feature_dim = base_model.fc.in_features  # 2048 for ResNet50
        
        # Attention fusion module to combine features from both branches
        self.attention_fusion = AttentionFusion(self.feature_dim)
        
        # Global average pooling to reduce spatial dimensions
        self.gap = nn.AdaptiveAvgPool2d(1)
        
        # MLP head with dropout for better generalization
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
        
        # Initialize weights of the classifier
        self._initialize_weights(self.classifier)
        
    def _initialize_weights(self, module):
        """Initialize weights of the model layers using Kaiming initialization."""
        for m in module.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                
    def forward(self, pre_images, post_images):
        """
        Forward pass of the model.
        Args:
            pre_images: Batch of pre-disaster images
            post_images: Batch of post-disaster images
        Returns:
            Classification logits for the damage categories
        """
        # Extract features from pre-disaster images
        pre_features = self.pre_branch(pre_images)
        
        # Extract features from post-disaster images
        post_features = self.post_branch(post_images)
        
        # Fuse features with attention mechanism
        fused_features = self.attention_fusion(pre_features, post_features)
        
        # Global average pooling
        pooled_features = self.gap(fused_features).view(-1, self.feature_dim)
        
        # Classification
        output = self.classifier(pooled_features)
        
        return output

# Functions for training
def seed_everything(seed=42):
    """
    Set random seeds for reproducibility across all libraries and components.
    This helps ensure consistent results between runs.
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"Random seed set to {seed}")

def compute_sample_weights(dataset, indices, weight_scale=1.0):
    """
    Compute sample weights for the given dataset indices to address class imbalance.
    
    Args:
        dataset: The dataset containing samples
        indices: Indices of the samples to use for weight calculation
        weight_scale: Controls the strength of class balancing (higher values give more weight to minority classes)
    
    Returns:
        sample_weights: Weights for each sample
        class_weights: Weights for each class
    """
    # Extract labels for the given indices
    labels = [dataset.samples[i]["label"] for i in indices]
    counts = Counter(labels)
    total = sum(counts.values())
    num_classes = len(DAMAGE_LABELS)
    
    # Compute inverse frequency weights
    # Gives higher weights to under-represented classes
    class_weights = {cls: (total / (num_classes * counts[cls]))**weight_scale for cls in counts}
    
    # Create sample weights
    sample_weights = [class_weights[label] for label in labels]
    
    return sample_weights, class_weights

def mixup_data(x1, x2, y, alpha=0.2, device='cuda'):
    """
    Applies mixup augmentation to the data.
    
    Mixup creates new training examples by linearly interpolating between two random inputs
    and their corresponding labels.
    
    Args:
        x1, x2: Pre and post disaster input tensors
        y: Target labels
        alpha: Parameter for Beta distribution
        device: Device to use
    
    Returns:
        Mixed inputs, original labels, and mixing coefficient
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x1.size()[0]
    index = torch.randperm(batch_size).to(device)

    mixed_x1 = lam * x1 + (1 - lam) * x1[index, :]
    mixed_x2 = lam * x2 + (1 - lam) * x2[index, :]
    y_a, y_b = y, y[index]
    
    return mixed_x1, mixed_x2, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """
    Criterion for mixup training - combines losses using the same mixing coefficient.
    
    Args:
        criterion: Loss function
        pred: Model predictions
        y_a, y_b: Original and permuted labels
        lam: Mixing coefficient
    
    Returns:
        Weighted average of losses for the two labels
    """
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def plot_learning_curves(epochs, train_losses, val_losses, val_accuracies, per_class_f1, class_names, save_path):
    """
    Plot and save training metrics visualizations.
    
    Args:
        epochs: List of epoch numbers
        train_losses: Training loss values for each epoch
        val_losses: Validation loss values for each epoch
        val_accuracies: Validation accuracy values for each epoch
        per_class_f1: F1 scores for each class across epochs
        class_names: Names of damage classes
        save_path: File path to save the plot
    """
    plt.figure(figsize=(16, 12))
    
    # Plot loss curves
    plt.subplot(2, 2, 1)
    plt.plot(epochs, train_losses, label="Train Loss", marker='o', color='blue')
    plt.plot(epochs, val_losses, label="Val Loss", marker='o', color='red')
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Plot validation accuracy
    plt.subplot(2, 2, 2)
    plt.plot(epochs, val_accuracies, label="Val Accuracy (%)", marker='o', color='green')
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.title("Validation Accuracy")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Plot per-class F1 scores
    plt.subplot(2, 2, 3)
    per_class_f1 = np.array(per_class_f1)  # shape: (num_epochs, num_classes)
    for i, cls_name in enumerate(class_names):
        plt.plot(epochs, per_class_f1[:, i], label=f"{cls_name}", marker='o')
    plt.xlabel("Epoch")
    plt.ylabel("F1 Score")
    plt.title("Per-Class F1 Scores")
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Plot class distribution of last batch
    plt.subplot(2, 2, 4)
    plt.bar(class_names, [per_class_f1[-1, i] for i in range(len(class_names))])
    plt.xlabel("Class")
    plt.ylabel("Final F1 Score")
    plt.title("Final F1 Score by Class")
    plt.xticks(rotation=45)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
    print(f"Learning curves plot saved to {save_path}")

class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance.
    
    Focal Loss down-weights easy examples (high confidence predictions) and focuses
    training on hard examples (low confidence), helping with class imbalance.
    
    Args:
        alpha: Optional weight for each class
        gamma: Focusing parameter (higher values increase focus on hard examples)
        reduction: How to reduce the loss ('mean', 'sum', or None)
    """
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha  # Weight for each class
        self.reduction = reduction

    def forward(self, inputs, targets):
        # Use nn.functional directly instead of F
        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma * ce_loss)
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss

def create_versioned_directory(base_path, prefix="trainingTry"):
    """
    Create a versioned directory with incremented try number if base exists.
    Helps keep track of multiple training runs without overwriting previous results.
    
    Args:
        base_path: Base directory path
        prefix: Prefix for the directory name
    
    Returns:
        full_path: Path to the created directory
        i: Version number
    """
    i = 1
    while True:
        dir_name = f"{prefix}{i}"
        full_path = os.path.join(base_path, dir_name)
        if not os.path.exists(full_path):
            os.makedirs(full_path, exist_ok=True)
            return full_path, i
        i += 1

def main():
    """
    Main function to run the training pipeline.
    Handles data loading, model training, evaluation, and saving results.
    """
    start_time = time.time()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    seed_everything(42)
    
    # Get project root directory
    project_root = os.path.abspath(os.path.dirname(__file__))
    # Go up two levels from scripts/training to the project root
    project_root = os.path.abspath(os.path.join(project_root, "..", ".."))
    
    # Hyperparameters & settings
    root_dir = os.path.join(project_root, "data", "xBD")
    batch_size = 32
    lr = 0.0001
    num_epochs = 20
    val_ratio = 0.15
    use_focal_loss = True
    use_mixup = False  # Disable mixup for now
    weight_scale = 0.7
    
    # Create versioned output directories
    base_output_dir = os.path.join(project_root, "output")
    os.makedirs(base_output_dir, exist_ok=True)
    output_dir, try_num = create_versioned_directory(base_output_dir)
    
    model_dir = os.path.join(output_dir, "models")
    os.makedirs(model_dir, exist_ok=True)
    pictures_dir = os.path.join(output_dir, "pictures")
    os.makedirs(pictures_dir, exist_ok=True)
    
    # Save configuration
    config = {
        "timestamp": timestamp,
        "batch_size": batch_size,
        "learning_rate": lr,
        "num_epochs": num_epochs,
        "val_ratio": val_ratio,
        "use_focal_loss": use_focal_loss,
        "use_mixup": use_mixup,
        "weight_scale": weight_scale,
    }
    
    with open(os.path.join(output_dir, f"config_try{try_num}.txt"), "w") as f:
        for key, value in config.items():
            f.write(f"{key}: {value}\n")

    # Set device (GPU if available, else CPU)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # Initialize dataset
    print("Initializing XBDPatchDataset for training...")
    full_dataset = XBDPatchDataset(
        root_dir=root_dir,
        pre_crop_size=128,
        post_crop_size=224,
        use_xy=True,
        max_samples=None
    )
    total_samples = len(full_dataset)
    print(f"Total samples: {total_samples}")

    # Create stratified train-val split directly
    def create_stratified_split(dataset, val_ratio=0.15, seed=42):
        """
        Create a stratified split of the data ensuring class balance between train and validation sets.
        Each class will have the same proportion of samples in both train and validation sets.
        """
        random.seed(seed)
        np.random.seed(seed)
        
        # Group samples by label
        label_to_indices = {}
        for idx, sample in enumerate(dataset.samples):
            label = sample['label']
            if label not in label_to_indices:
                label_to_indices[label] = []
            label_to_indices[label].append(idx)
        
        train_indices = []
        val_indices = []
        
        # Stratified sampling - take val_ratio proportion from each class
        for label, indices in label_to_indices.items():
            random.shuffle(indices)
            val_size = int(len(indices) * val_ratio)
            val_indices.extend(indices[:val_size])
            train_indices.extend(indices[val_size:])
        
        # Shuffle the indices to avoid any ordering bias
        random.shuffle(train_indices)
        random.shuffle(val_indices)
        
        return train_indices, val_indices
    
    # Create the split
    train_indices, val_indices = create_stratified_split(full_dataset, val_ratio, seed=42)
    print(f"Training samples: {len(train_indices)}, Validation samples: {len(val_indices)}")

    # Compute class weights for weighted sampling
    sample_weights, class_weights = compute_sample_weights(full_dataset, train_indices, weight_scale)
    sampler = SubsetRandomSampler(train_indices)
    
    # Create train and validation datasets
    train_dataset = Subset(full_dataset, train_indices)
    val_dataset = Subset(full_dataset, val_indices)
    
    # Compute sample weights for weighted sampling
    # This helps address class imbalance during training
    sample_weights, class_weights = compute_sample_weights(full_dataset, train_indices, weight_scale)
    sampler = torch.utils.data.WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        sampler=sampler,  # Use weighted sampler for balanced class distribution
        num_workers=16,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size,
        shuffle=False,  # No need for sampler with validation set
        num_workers=16,
        pin_memory=True
    )

    # Initialize model - check if pretrained is a valid parameter
    try:
        model = BaselineModel(num_classes=4, pretrained=True, dropout_rate=0.5).to(device)
    except TypeError:
        # If pretrained is not a valid parameter, try without it
        print("Warning: 'pretrained' parameter not supported, using default initialization")
        model = BaselineModel(num_classes=4).to(device)
    
    # Define class weights for loss function
    weight_tensor = torch.tensor([class_weights.get(i, 1.0) for i in range(4)], dtype=torch.float).to(device)
    
    # Loss function
    if use_focal_loss:
        criterion = FocalLoss(alpha=weight_tensor, gamma=2.0, reduction='mean')
        print("Using Focal Loss with gamma=2.0")
    else:
        criterion = nn.CrossEntropyLoss(weight=weight_tensor)
        print("Using Weighted CrossEntropyLoss")
    
    # Optimizer with weight decay
    # AdamW provides better regularization than standard Adam
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    
    # Learning rate scheduler
    # OneCycleLR gradually increases then decreases learning rate
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, 
        max_lr=lr,
        epochs=num_epochs,
        steps_per_epoch=len(train_loader),
        pct_start=0.3,  # Percentage of training to increase LR
        anneal_strategy='cos'  # Use cosine annealing
    )

    # Training metrics tracking
    epochs_list = []
    train_losses = []
    val_losses = []
    val_accuracies = []
    per_class_f1_scores = []
    best_f1_score = 0.0
    best_model_path = None  # Track the path of the best model
    
    # Training loop
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        epoch_start_time = time.time()
        
        # Training phase
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [TRAIN]", leave=True)
        for pre_batch, post_batch, labels in train_pbar:
            pre_batch = pre_batch.to(device)
            post_batch = post_batch.to(device)
            labels = labels.to(device)
            
            # Apply mixup if enabled
            if use_mixup and np.random.random() < 0.5:
                pre_batch, post_batch, labels_a, labels_b, lam = mixup_data(pre_batch, post_batch, labels, alpha=0.2, device=device)
                mixup_applied = True
            else:
                mixup_applied = False

            # Zero gradients before forward pass
            optimizer.zero_grad()
            # Forward pass
            outputs = model(pre_batch, post_batch)
            
            # Compute loss based on whether mixup was applied
            if mixup_applied:
                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)
            else:
                loss = criterion(outputs, labels)
                
            # Backward pass and optimization
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            running_loss += loss.item()
            train_pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
        # Calculate average training loss for the epoch
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validation phase
        model.eval()
        val_loss_epoch = 0.0
        correct = 0
        total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [VAL]", leave=True)
            for pre_batch, post_batch, labels in val_pbar:
                pre_batch = pre_batch.to(device)
                post_batch = post_batch.to(device)
                labels = labels.to(device)
                
                # Forward pass
                outputs = model(pre_batch, post_batch)
                loss = criterion(outputs, labels)
                val_loss_epoch += loss.item()
                
                # Get predictions and calculate accuracy
                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)
                
                # Collect predictions and labels for F1 score calculation
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                val_pbar.set_postfix({"loss": f"{loss.item():.4f}"})
                
        # Calculate average validation loss
        val_loss_epoch /= len(val_loader)
        val_losses.append(val_loss_epoch)
        val_acc = 100 * correct / total if total > 0 else 0
        val_accuracies.append(val_acc)
        
        # Calculate F1 scores for each class
        epoch_f1 = f1_score(all_labels, all_preds, average=None, labels=[0, 1, 2, 3])
        per_class_f1_scores.append(epoch_f1)
        
        # Calculate macro F1 for model saving
        macro_f1 = np.mean(epoch_f1)
        epochs_list.append(epoch + 1)
        
        # Calculate elapsed time
        epoch_time = time.time() - epoch_start_time
        
        # Print epoch summary
        print(f"Epoch [{epoch+1}/{num_epochs}] "
              f"Train Loss: {avg_train_loss:.4f} | "
              f"Val Loss: {val_loss_epoch:.4f} | "
              f"Val Acc: {val_acc:.2f}% | "
              f"Macro F1: {macro_f1:.4f} | "
              f"Time: {epoch_time:.1f}s")
        
        print(f"Per-class F1 Scores: {', '.join([f'{c}: {f:.4f}' for c, f in zip(DAMAGE_LABELS.keys(), epoch_f1)])}")
        
        # Save the model if it's the best so far, and delete previous best
        if macro_f1 > best_f1_score:
            # Delete previous best model file if it exists
            if best_model_path and os.path.exists(best_model_path):
                os.remove(best_model_path)
                print(f"Removed previous best model: {best_model_path}")
                
            best_f1_score = macro_f1
            best_model_path = os.path.join(model_dir, f"best_model_epoch_{epoch+1}.pt")
            torch.save(model.state_dict(), best_model_path)
            print(f"New best model saved with Macro F1: {best_f1_score:.4f}")

    # Copy the best model as baseline_best.pt (instead of saving a new copy in the root)
    if best_model_path and os.path.exists(best_model_path):
        baseline_best_path = os.path.join(model_dir, "baseline_best.pt")
        shutil.copy2(best_model_path, baseline_best_path)
        print(f"Best model copied to {baseline_best_path}")
    
    # Create learning curves plot
    plot_save_path = os.path.join(pictures_dir, "learning_curves.png")
    plot_learning_curves(
        epochs_list, 
        train_losses, 
        val_losses, 
        val_accuracies, 
        per_class_f1_scores,
        list(DAMAGE_LABELS.keys()),
        plot_save_path
    )
    
    # Save all training metrics
    metrics = {
        "epochs": epochs_list,
        "train_losses": train_losses,
        "val_losses": val_losses,
        "val_accuracies": val_accuracies,
        "per_class_f1_scores": per_class_f1_scores,
    }
    
    metrics_path = os.path.join(output_dir, f"training_metrics_try{try_num}.txt")
    with open(metrics_path, "w") as f:
        for key, values in metrics.items():
            if key == "per_class_f1_scores":
                f.write(f"{key}:\n")
                for epoch_idx, epoch_scores in enumerate(values):
                    f.write(f"  Epoch {epoch_idx+1}: {epoch_scores}\n")
            else:
                f.write(f"{key}: {values}\n")
    
    total_time = time.time() - start_time
    print(f"Training completed in {total_time/60:.2f} minutes")
    print(f"Best validation Macro F1: {best_f1_score:.4f}")
    print(f"All training artifacts saved to {output_dir}")
    print("Run 'evaluate_baseline.py' to test the model")

if __name__ == "__main__":
    main()



evaluate_baseline:
#!/usr/bin/env python3
"""
Evaluation script for the baseline building damage assessment model.

This script:
1. Loads a trained model
2. Evaluates it on the test dataset
3. Computes detailed metrics and visualizations
4. Generates comprehensive reports
"""
import os
import sys
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (confusion_matrix, classification_report,
                             f1_score, precision_score, recall_score,
                             cohen_kappa_score, balanced_accuracy_score,
                             accuracy_score, roc_auc_score, roc_curve, auc)
import time
from datetime import datetime
import json
from PIL import Image
import torchvision.transforms as T
from shapely import wkt
import glob

# Constants and definitions
DAMAGE_LABELS = {
    "no-damage": 0,
    "minor-damage": 1,
    "major-damage": 2,
    "destroyed": 3
}

class XBDPatchDataset(Dataset):
    """
    Dataset for the xBD satellite imagery dataset for building damage assessment.
    
    This dataset:
    1. Loads pre and post-disaster satellite images
    2. Extracts building polygons from JSON labels
    3. Crops images around building locations
    4. Returns (pre_image, post_image, damage_label) for each building
    
    Supports both flat and hierarchical data structures:
    - Flat: root_dir contains 'images' and 'labels' folders directly
    - Hierarchical: root_dir contains subfolders per disaster, each with 'images' and 'labels'
    """
    def __init__(self,
                 root_dir,
                 pre_crop_size=128,
                 post_crop_size=224,
                 use_xy=True,
                 max_samples=None,
                 flat_structure=False,
                 augment=False):
        """
        Initialize the dataset.
        
        Args:
            root_dir (str): Directory containing the dataset
            pre_crop_size (int): Size to crop pre-disaster images
            post_crop_size (int): Size to crop post-disaster images
            use_xy (bool): Use 'xy' coordinates if True, else 'lng_lat'
            max_samples (int): Limit number of samples (for testing/debugging)
            flat_structure (bool): If True, root_dir has images/ and labels/ directly
            augment (bool): Apply data augmentation if True
        """
        super().__init__()
        self.root_dir = root_dir
        self.pre_crop_size = pre_crop_size
        self.post_crop_size = post_crop_size
        self.coord_key = "xy" if use_xy else "lng_lat"
        self.max_samples = max_samples
        self.flat_structure = flat_structure
        self.augment = augment

        # Define image preprocessing transforms
        self.pre_transform = T.Compose([
            T.Resize((pre_crop_size, pre_crop_size)),
            T.ToTensor(),
            # ImageNet normalization values
            T.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
        ])
        
        self.post_transform = T.Compose([
            T.Resize((post_crop_size, post_crop_size)),
            T.ToTensor(),
            # ImageNet normalization values
            T.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
        ])

        # Gather all valid samples from the dataset
        self.samples = self._gather_samples()
        
        # Limit samples if requested (for testing/debugging)
        if self.max_samples is not None and len(self.samples) > self.max_samples:
            self.samples = random.sample(self.samples, self.max_samples)

    def _gather_samples(self):
        """
        Scan the dataset directory and gather all valid samples.
        
        This method handles both flat and hierarchical directory structures.
        For each building in the dataset, it:
        1. Reads the JSON label file
        2. Extracts building polygons
        3. Gets damage labels
        4. Computes bounding boxes
        
        Returns:
            list: List of dictionaries containing sample metadata
        """
        samples = []
        
        # First check if we have a flat structure (images/ and labels/ directly)
        images_dir = os.path.join(self.root_dir, "images")
        labels_dir = os.path.join(self.root_dir, "labels")
        
        if os.path.isdir(images_dir) and os.path.isdir(labels_dir):
            # This looks like a flat structure
            print(f"Detected flat structure at {self.root_dir}")
            label_files = [f for f in os.listdir(labels_dir) if f.endswith("_post_disaster.json")]
            
            for label_file in label_files:
                # Extract base ID from filename (remove _post_disaster.json)
                base_id = label_file.replace("_post_disaster.json", "")
                post_img_name = base_id + "_post_disaster.png"
                pre_img_name = base_id + "_pre_disaster.png"
                post_json_path = os.path.join(labels_dir, label_file)
                post_img_path = os.path.join(images_dir, post_img_name)
                pre_json_path = os.path.join(labels_dir, base_id + "_pre_disaster.json")
                pre_img_path = os.path.join(images_dir, pre_img_name)
                
                # Skip if any files are missing
                if not (os.path.isfile(post_json_path) and os.path.isfile(post_img_path)
                        and os.path.isfile(pre_json_path) and os.path.isfile(pre_img_path)):
                    continue
                    
                # Read the JSON file with building annotations
                with open(post_json_path, 'r') as f:
                    post_data = json.load(f)
                    
                # Extract building polygons and their damage labels
                feats = post_data.get("features", {}).get(self.coord_key, [])
                for feat in feats:
                    damage_type = feat.get("properties", {}).get("subtype", "").lower()
                    if damage_type not in DAMAGE_LABELS:
                        continue
                        
                    label = DAMAGE_LABELS[damage_type]
                    wkt_str = feat.get("wkt", None)
                    if wkt_str is None:
                        continue
                        
                    # Convert WKT to polygon and get bounding box
                    polygon = wkt.loads(wkt_str)
                    minx, miny, maxx, maxy = polygon.bounds
                    samples.append({
                        "pre_img": pre_img_path,
                        "post_img": post_img_path,
                        "bbox": (minx, miny, maxx, maxy),
                        "label": label
                    })
        else:
            # Hierarchical structure: root_dir contains subfolders per disaster
            try:
                # Get all disaster directories (excluding any special directories)
                disasters = [d for d in os.listdir(self.root_dir)
                            if os.path.isdir(os.path.join(self.root_dir, d))
                            and d.lower() != "spacenet_gt"]
                
                print(f"Found {len(disasters)} disaster folders")
                
                for disaster in disasters:
                    disaster_dir = os.path.join(self.root_dir, disaster)
                    images_dir = os.path.join(disaster_dir, "images")
                    labels_dir = os.path.join(disaster_dir, "labels")
                    
                    if not (os.path.isdir(images_dir) and os.path.isdir(labels_dir)):
                        print(f"Warning: Missing images or labels directory for disaster: {disaster}")
                        continue
                        
                    # Get all post-disaster JSON files
                    label_files = [f for f in os.listdir(labels_dir) if f.endswith("_post_disaster.json")]
                    print(f"Disaster {disaster}: Found {len(label_files)} label files")
                    
                    for label_file in label_files:
                        # Extract base ID from filename
                        base_id = label_file.replace("_post_disaster.json", "")
                        post_img_name = base_id + "_post_disaster.png"
                        pre_img_name = base_id + "_pre_disaster.png"
                        post_json_path = os.path.join(labels_dir, label_file)
                        post_img_path = os.path.join(images_dir, post_img_name)
                        pre_json_path = os.path.join(labels_dir, base_id + "_pre_disaster.json")
                        pre_img_path = os.path.join(images_dir, pre_img_name)
                        
                        # Skip if any files are missing
                        if not (os.path.isfile(post_json_path) and os.path.isfile(post_img_path)
                                and os.path.isfile(pre_json_path) and os.path.isfile(pre_img_path)):
                            continue
                            
                        # Read the JSON file with building annotations
                        with open(post_json_path, 'r') as f:
                            post_data = json.load(f)
                            
                        # Extract building polygons and their damage labels
                        feats = post_data.get("features", {}).get(self.coord_key, [])
                        for feat in feats:
                            damage_type = feat.get("properties", {}).get("subtype", "").lower()
                            if damage_type not in DAMAGE_LABELS:
                                continue
                                
                            label = DAMAGE_LABELS[damage_type]
                            wkt_str = feat.get("wkt", None)
                            if wkt_str is None:
                                continue
                                
                            # Convert WKT to polygon and get bounding box
                            polygon = wkt.loads(wkt_str)
                            minx, miny, maxx, maxy = polygon.bounds
                            samples.append({
                                "pre_img": pre_img_path,
                                "post_img": post_img_path,
                                "bbox": (minx, miny, maxx, maxy),
                                "label": label,
                                "disaster": disaster  # Track source disaster
                            })
            except Exception as e:
                print(f"Error gathering samples: {e}")
                
        print(f"Total gathered samples: {len(samples)}")
        
        # Print class distribution statistics
        labels = [s["label"] for s in samples]
        unique_labels, counts = np.unique(labels, return_counts=True)
        print("Class distribution:")
        for label, count in zip(unique_labels, counts):
            class_name = [name for name, idx in DAMAGE_LABELS.items() if idx == label][0]
            print(f"  {class_name}: {count} samples ({count/len(samples)*100:.2f}%)")
            
        return samples

    def __len__(self):
        """Return the total number of samples in the dataset."""
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Get a single sample (pre_image, post_image, label) at the given index.
        
        This method:
        1. Loads pre and post-disaster images
        2. Crops around the building
        3. Applies transforms and augmentation (if enabled)
        
        Args:
            idx (int): Sample index
            
        Returns:
            tuple: (pre_tensor, post_tensor, label)
        """
        item = self.samples[idx]
        pre_path = item["pre_img"]
        post_path = item["post_img"]
        (minx, miny, maxx, maxy) = item["bbox"]
        label = item["label"]

        try:
            # Load images and convert to RGB
            pre_img = Image.open(pre_path).convert("RGB")
            post_img = Image.open(post_path).convert("RGB")

            # Crop around the building
            pre_crop = self._center_crop(pre_img, minx, miny, maxx, maxy, self.pre_crop_size)
            post_crop = self._center_crop(post_img, minx, miny, maxx, maxy, self.post_crop_size)

            # Use consistent random state for both images when augmenting
            if self.augment:
                seed = np.random.randint(2147483647)
                random.seed(seed)
                torch.manual_seed(seed)
                
            # Apply transformations
            pre_tensor = self.pre_transform(pre_crop)
            
            # Reset seed to ensure same transforms for post-image if augmenting
            if self.augment:
                random.seed(seed)
                torch.manual_seed(seed)
                
            post_tensor = self.post_transform(post_crop)

            return pre_tensor, post_tensor, label
            
        except Exception as e:
            print(f"Error processing item {idx}: {e}")
            # Return a placeholder in case of error
            placeholder_pre = torch.zeros(3, self.pre_crop_size, self.pre_crop_size)
            placeholder_post = torch.zeros(3, self.post_crop_size, self.post_crop_size)
            return placeholder_pre, placeholder_post, label

    def _center_crop(self, pil_img, minx, miny, maxx, maxy, crop_size):
        """
        Crop an image centered on a building.
        
        Args:
            pil_img (PIL.Image): Image to crop
            minx, miny, maxx, maxy (float): Bounding box coordinates
            crop_size (int): Size of the square crop
            
        Returns:
            PIL.Image: Cropped image
        """
        width, height = pil_img.size
        
        # Calculate center of the bounding box
        bb_width = maxx - minx
        bb_height = maxy - miny
        cx = minx + bb_width / 2.0
        cy = miny + bb_height / 2.0
        
        # Calculate crop boundaries, ensuring they stay within image
        half = crop_size / 2.0
        left = max(0, min(cx - half, width - crop_size))
        top = max(0, min(cy - half, height - crop_size))
        right = left + crop_size
        bottom = top + crop_size
        
        return pil_img.crop((left, top, right, bottom))

class AttentionFusion(nn.Module):
    """
    Attention Fusion module for combining features from pre and post-disaster branches.
    
    This module:
    1. Concatenates pre and post features
    2. Computes attention weights
    3. Applies attention to post features
    4. Fuses pre and weighted post features
    """
    def __init__(self, in_channels):
        """
        Initialize the attention fusion module.
        
        Args:
            in_channels (int): Number of input feature channels
        """
        super(AttentionFusion, self).__init__()
        
        # Attention mechanism as a small CNN
        self.attention = nn.Sequential(
            nn.Conv2d(in_channels * 2, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, 1, kernel_size=1),
            nn.Sigmoid()  # Output attention weights between 0-1
        )
        
    def forward(self, pre_feat, post_feat):
        """
        Forward pass of the attention fusion module.
        
        Args:
            pre_feat (Tensor): Features from pre-disaster branch
            post_feat (Tensor): Features from post-disaster branch
            
        Returns:
            Tensor: Fused features
        """
        # Ensure spatial dimensions match before concatenating
        if pre_feat.shape[2:] != post_feat.shape[2:]:
            # Resize post_feat to match pre_feat's spatial dimensions
            post_feat = F.interpolate(post_feat, size=pre_feat.shape[2:], 
                                      mode='bilinear', align_corners=False)
            
        # Concatenate features along channel dimension
        concat_feat = torch.cat([pre_feat, post_feat], dim=1)
        
        # Generate attention weights
        attn_weights = self.attention(concat_feat)
        
        # Apply attention to post features
        weighted_post = post_feat * attn_weights
        
        # Combine pre and weighted post features
        fused_feat = pre_feat + weighted_post
        
        return fused_feat

class BaselineModel(nn.Module):
    """
    Baseline model for building damage assessment using pre and post-disaster images.
    
    Architecture:
    - Dual ResNet50 branches for pre and post-disaster images
    - Attention fusion module to combine features
    - MLP classifier head
    """
    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.5):
        """
        Initialize the model.
        
        Args:
            num_classes (int): Number of damage categories
            pretrained (bool): Whether to use pretrained weights
            dropout_rate (float): Dropout rate for regularization
        """
        super(BaselineModel, self).__init__()
        
        # Use ResNet50 as backbone
        try:
            # For torch 1.13+
            import torchvision.models as models
            base_model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)
        except TypeError:
            # For older torch versions
            import torchvision.models as models
            base_model = models.resnet50(pretrained=pretrained)
        
        # Pre-disaster branch - use all layers except final FC
        self.pre_branch = nn.Sequential(*list(base_model.children())[:-2])
        
        # Post-disaster branch - same architecture but separate weights
        try:
            # For torch 1.13+
            import torchvision.models as models
            post_model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)
        except TypeError:
            # For older torch versions
            import torchvision.models as models
            post_model = models.resnet50(pretrained=pretrained)
        
        self.post_branch = nn.Sequential(*list(post_model.children())[:-2])
        
        # Number of features from ResNet50 (2048)
        self.feature_dim = base_model.fc.in_features
        
        # Attention fusion module to combine features from both branches
        self.attention_fusion = AttentionFusion(self.feature_dim)
        
        # Global average pooling to reduce spatial dimensions
        self.gap = nn.AdaptiveAvgPool2d(1)
        
        # MLP classifier head with dropout for better generalization
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
        
        # Initialize weights of the classifier
        self._initialize_weights(self.classifier)
        
    def _initialize_weights(self, module):
        """
        Initialize weights for the given module using Kaiming initialization.
        
        Args:
            module (nn.Module): Module to initialize
        """
        for m in module.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                
    def forward(self, pre_images, post_images):
        """
        Forward pass of the model.
        
        Args:
            pre_images (Tensor): Pre-disaster images
            post_images (Tensor): Post-disaster images
            
        Returns:
            Tensor: Class logits
        """
        # Extract features from pre-disaster images
        pre_features = self.pre_branch(pre_images)
        
        # Extract features from post-disaster images
        post_features = self.post_branch(post_images)
        
        # Fuse features with attention mechanism
        fused_features = self.attention_fusion(pre_features, post_features)
        
        # Global average pooling to reduce spatial dimensions
        pooled_features = self.gap(fused_features).view(-1, self.feature_dim)
        
        # Classification
        output = self.classifier(pooled_features)
        
        return output

# Evaluation functions
def seed_everything(seed=42):
    """
    Set random seeds for reproducibility across all libraries.
    
    Args:
        seed (int): Random seed value
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"Random seed set to {seed}")

def test_time_augmentation(model, pre_img, post_img, device, num_augments=5):
    """
    Apply test-time augmentation (TTA) to improve prediction robustness.
    
    TTA runs multiple forward passes with different augmentations
    and averages the predictions.
    
    Args:
        model (nn.Module): Model to evaluate
        pre_img (Tensor): Pre-disaster image batch
        post_img (Tensor): Post-disaster image batch
        device (str): Device to use
        num_augments (int): Number of augmentations to apply
        
    Returns:
        Tensor: Averaged predictions
    """
    model.eval()
    batch_size = pre_img.size(0)
    
    # List to store all predictions
    all_outputs = []
    
    # Original prediction (no augmentation)
    with torch.no_grad():
        outputs_original = model(pre_img, post_img)
        all_outputs.append(outputs_original)
    
    # Horizontal flip augmentation
    with torch.no_grad():
        pre_hflip = torch.flip(pre_img, dims=[3])
        post_hflip = torch.flip(post_img, dims=[3])
        outputs_hflip = model(pre_hflip, post_hflip)
        all_outputs.append(outputs_hflip)
    
    # Vertical flip augmentation
    with torch.no_grad():
        pre_vflip = torch.flip(pre_img, dims=[2])
        post_vflip = torch.flip(post_img, dims=[2])
        outputs_vflip = model(pre_vflip, post_vflip)
        all_outputs.append(outputs_vflip)
    
    # Both horizontal and vertical flip
    with torch.no_grad():
        pre_hvflip = torch.flip(pre_img, dims=[2, 3])
        post_hvflip = torch.flip(post_img, dims=[2, 3])
        outputs_hvflip = model(pre_hvflip, post_hvflip)
        all_outputs.append(outputs_hvflip)
    
    # Center crop augmentation (90% of image)
    if num_augments > 4:
        with torch.no_grad():
            h, w = pre_img.shape[2:]
            ch, cw = int(h * 0.1), int(w * 0.1)
            pre_crop = pre_img[:, :, ch:h-ch, cw:w-cw]
            post_crop = post_img[:, :, ch:h-ch, cw:w-cw]
            pre_crop = F.interpolate(pre_crop, size=(h, w), mode='bilinear', align_corners=False)
            post_crop = F.interpolate(post_crop, size=(h, w), mode='bilinear', align_corners=False)
            outputs_crop = model(pre_crop, post_crop)
            all_outputs.append(outputs_crop)
    
    # Average the predictions
    outputs = torch.stack(all_outputs).mean(dim=0)
    return outputs

def evaluate_model(model, dataloader, device, use_tta=False):
    """
    Evaluate model on the given dataloader.
    
    Args:
        model (nn.Module): Model to evaluate
        dataloader (DataLoader): DataLoader for evaluation
        device (str): Device to use
        use_tta (bool): Whether to use test-time augmentation
        
    Returns:
        tuple: (accuracy, predictions, labels, probabilities)
    """
    model.eval()
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    all_probs = []  # For ROC curves
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Evaluating", leave=True)
        for pre_imgs, post_imgs, labels in pbar:
            pre_imgs = pre_imgs.to(device)
            post_imgs = post_imgs.to(device)
            labels = labels.to(device)
            
            # Either use test-time augmentation or standard forward pass
            if use_tta:
                outputs = test_time_augmentation(model, pre_imgs, post_imgs, device)
            else:
                outputs = model(pre_imgs, post_imgs)
            
            # Get predictions and probabilities
            probabilities = F.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)
            
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probabilities.cpu().numpy())
            
            # Update progress bar
            pbar.set_postfix({"Acc": f"{100 * correct / total:.2f}%"})
    
    accuracy = 100 * correct / total if total > 0 else 0
    return accuracy, np.array(all_preds), np.array(all_labels), np.array(all_probs)

def plot_confusion_matrix(cm, target_names, save_path, normalize=False, title=None):
    """
    Generate and save a confusion matrix plot.
    
    Args:
        cm (ndarray): Confusion matrix from sklearn
        target_names (list): List of class names
        save_path (str): Path to save the plot
        normalize (bool): Whether to normalize values
        title (str): Title for the plot
    """
    if normalize:
        cm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-6)
        fmt = '.2f'
    else:
        fmt = 'd'
    
    plt.figure(figsize=(10, 8))
    sns.set(font_scale=1.2)
    sns.heatmap(cm, annot=True, fmt=fmt, cmap="Blues",
                xticklabels=target_names, yticklabels=target_names)
    
    plt.ylabel('True Label', fontsize=14)
    plt.xlabel('Predicted Label', fontsize=14)
    
    if title:
        plt.title(title, fontsize=16)
    else:
        plt.title("Confusion Matrix", fontsize=16)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

def plot_per_class_metrics(metrics, metric_name, target_names, save_path):
    """
    Generate and save per-class metrics bar chart.
    
    Args:
        metrics (ndarray): Array of metric values for each class
        metric_name (str): Name of the metric
        target_names (list): List of class names
        save_path (str): Path to save the plot
    """
    plt.figure(figsize=(12, 6))
    x = np.arange(len(target_names))
    
    bar_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    bars = plt.bar(x, metrics, color=bar_colors, width=0.6)
    
    # Add value labels on top of each bar
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', fontsize=10)
    
    plt.axhline(y=np.mean(metrics), color='r', linestyle='--', label=f'Mean: {np.mean(metrics):.3f}')
    
    plt.xticks(x, target_names, rotation=30)
    plt.ylim(0, min(1, max(metrics) + 0.15))
    
    plt.xlabel("Classes", fontsize=12)
    plt.ylabel(metric_name, fontsize=12)
    plt.title(f"Per-Class {metric_name}", fontsize=14)
    
    plt.legend()
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

def plot_roc_curves(all_probs, all_labels, target_names, save_path):
    """
    Generate and save ROC curves for each class.
    
    Args:
        all_probs (ndarray): Predicted probabilities for each class
        all_labels (ndarray): True labels
        target_names (list): List of class names
        save_path (str): Path to save the plot
    """
    plt.figure(figsize=(12, 8))
    
    # One-hot encode the labels for multi-class ROC
    n_classes = len(target_names)
    y_true_onehot = np.eye(n_classes)[all_labels]
    
    # Colors for each class
    colors = ['blue', 'orange', 'green', 'red']
    
    # Calculate ROC curve and ROC area for each class
    for i, color, target_name in zip(range(n_classes), colors, target_names):
        fpr, tpr, _ = roc_curve(y_true_onehot[:, i], all_probs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=color, lw=2,
                 label=f'{target_name} (AUC = {roc_auc:.3f})')
    
    # Plot the diagonal
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=14)
    plt.legend(loc="lower right")
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()

def analyze_errors(all_preds, all_labels, all_probs, target_names, save_path):
    """
    Analyze prediction errors to find common patterns.
    
    This function:
    1. Calculates error rates for each class
    2. Finds common misclassification patterns
    3. Analyzes confidence distributions
    
    Args:
        all_preds (ndarray): Model predictions
        all_labels (ndarray): True labels
        all_probs (ndarray): Prediction probabilities
        target_names (list): List of class names
        save_path (str): Path to save the plot
    """
    # Create a figure for error analysis
    plt.figure(figsize=(12, 8))
    
    # Get indices of errors and correct predictions
    error_idx = np.where(all_preds != all_labels)[0]
    correct_idx = np.where(all_preds == all_labels)[0]
    
    # Calculate error rates for each true class
    error_rates = []
    error_types = []
    
    for true_class in range(len(target_names)):
        # Get all samples of this true class
        class_samples = np.where(all_labels == true_class)[0]
        # Find errors among these samples
        class_errors = np.intersect1d(class_samples, error_idx)
        # Calculate error rate
        error_rate = len(class_errors) / len(class_samples) if len(class_samples) > 0 else 0
        error_rates.append(error_rate)
        
        # Find common misclassifications
        if len(class_errors) > 0:
            error_preds = all_preds[class_errors]
            unique_preds, counts = np.unique(error_preds, return_counts=True)
            most_common_idx = np.argmax(counts)
            most_common_class = unique_preds[most_common_idx]
            error_types.append(f"{target_names[true_class]} → {target_names[most_common_class]}")
        else:
            error_types.append("No errors")
    
    # Plot error rates
    plt.subplot(2, 1, 1)
    bars = plt.bar(target_names, error_rates, color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'])
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{height:.2f}', ha='center', fontsize=10)
    
    plt.title("Error Rate by True Class", fontsize=14)
    plt.ylabel("Error Rate", fontsize=12)
    plt.ylim(0, max(error_rates) + 0.1)
    
    # Analyze confidence of correct vs. incorrect predictions
    plt.subplot(2, 1, 2)
    
    # Get confidence (probability of predicted class)
    confidences = np.array([all_probs[i, pred] for i, pred in enumerate(all_preds)])
    
    # Split by correct/incorrect
    correct_conf = confidences[correct_idx]
    error_conf = confidences[error_idx]
    
    # Create histogram
    plt.hist(correct_conf, bins=20, alpha=0.5, label=f'Correct (n={len(correct_conf)})', color='green')
    plt.hist(error_conf, bins=20, alpha=0.5, label=f'Incorrect (n={len(error_conf)})', color='red')
    
    plt.xlabel("Prediction Confidence", fontsize=12)
    plt.ylabel("Count", fontsize=12)
    plt.title("Confidence Distribution for Correct vs. Incorrect Predictions", fontsize=14)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    
    # Print error analysis summary
    print("\nError Analysis:")
    print("  Most common misclassification patterns:")
    for i, error in enumerate(error_types):
        print(f"  - {error} (Error rate: {error_rates[i]:.2f})")

def create_versioned_directory(base_path, prefix="evaluationTry"):
    """
    Create a versioned directory with incremented try number.
    
    Args:
        base_path (str): Base directory path
        prefix (str): Prefix for directory name
        
    Returns:
        tuple: (directory_path, try_number)
    """
    i = 1
    while True:
        dir_name = f"{prefix}{i}"
        full_path = os.path.join(base_path, dir_name)
        if not os.path.exists(full_path):
            os.makedirs(full_path, exist_ok=True)
            return full_path, i
        i += 1

def find_best_model(models_dir):
    """
    Find the latest best model in the models directory.
    
    This function first looks for baseline_best.pt, and if not found,
    finds the best_model_epoch_X.pt file with the highest epoch number.
    
    Args:
        models_dir (str): Path to the models directory
        
    Returns:
        str: Path to the best model
    """
    # First check for baseline_best.pt
    baseline_path = os.path.join(models_dir, "baseline_best.pt")
    if os.path.exists(baseline_path):
        return baseline_path
    
    # Otherwise, look for the best_model_epoch_X.pt file with the highest epoch number
    model_files = glob.glob(os.path.join(models_dir, "best_model_epoch_*.pt"))
    if not model_files:
        return None
    
    # Extract epoch numbers and find the highest
    epoch_numbers = []
    for file_path in model_files:
        filename = os.path.basename(file_path)
        try:
            epoch = int(filename.replace("best_model_epoch_", "").replace(".pt", ""))
            epoch_numbers.append((epoch, file_path))
        except ValueError:
            continue
    
    if not epoch_numbers:
        return None
    
    # Return the path with the highest epoch number
    return sorted(epoch_numbers, key=lambda x: x[0], reverse=True)[0][1]

def main():
    """
    Main evaluation function.
    
    This function:
    1. Sets up the environment (seed, directories)
    2. Loads the best trained model
    3. Evaluates it on the test dataset
    4. Computes detailed metrics
    5. Generates visualizations and reports
    """
    # Set random seed for reproducibility
    seed_everything(42)
    
    # Get project root directory
    project_root = os.path.abspath(os.path.dirname(__file__))
    # Go up two levels from scripts/training to the project root
    project_root = os.path.abspath(os.path.join(project_root, "..", ".."))
    
    # Configuration
    use_tta = True  # Use test-time augmentation for more robust predictions
    
    # Find the latest training directory and best model within it
    output_dir = os.path.join(project_root, "output")
    training_dirs = sorted([d for d in os.listdir(output_dir) 
                           if os.path.isdir(os.path.join(output_dir, d)) and d.startswith("trainingTry")],
                          key=lambda x: int(x.replace("trainingTry", "")))
    
    if not training_dirs:
        print("No training directories found. Please train a model first.")
        return
    
    # Get the latest training directory
    latest_training_dir = os.path.join(output_dir, training_dirs[-1])
    models_dir = os.path.join(latest_training_dir, "models")
    
    # Find the best model in the latest training directory
    best_model_path = find_best_model(models_dir)
    
    if not best_model_path:
        print(f"No model checkpoint found in {models_dir}")
        return
    
    print(f"Using model: {best_model_path}")
    
    # Set device (GPU if available, else CPU)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # Create timestamp for saving results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create versioned results directory
    base_results_dir = os.path.join(project_root, "evaluation_results")
    os.makedirs(base_results_dir, exist_ok=True)
    results_dir, try_num = create_versioned_directory(base_results_dir)
    
    # Test data is stored in a hierarchical structure under data/test
    test_data_dir = os.path.join(project_root, "data", "test")
    print(f"Loading evaluation data from: {test_data_dir}")

    # Create test dataset without augmentation
    test_dataset = XBDPatchDataset(
        root_dir=test_data_dir,
        pre_crop_size=128,
        post_crop_size=224,
        use_xy=True,
        max_samples=None,  # Use all samples for evaluation
        flat_structure=False,  # Test data is hierarchical
        augment=False        # No augmentation during evaluation
    )
    print(f"Total evaluation samples: {len(test_dataset)}")

    # Create test dataloader
    test_loader = DataLoader(
        test_dataset, 
        batch_size=32,  # Larger batch size for faster evaluation
        shuffle=False,  # Don't shuffle for evaluation
        num_workers=4,  # Parallel data loading
        pin_memory=True,  # Faster data transfer to GPU
        persistent_workers=True  # Keep workers alive between iterations
    )

    # Initialize model
    model = BaselineModel(num_classes=4).to(device)
    
    # Load checkpoint
    if os.path.isfile(best_model_path):
        checkpoint = torch.load(best_model_path, map_location=device)
        model.load_state_dict(checkpoint, strict=False)
        print(f"Loaded model weights from {best_model_path}")
    else:
        print(f"Model checkpoint not found at {best_model_path}")
        return

    # Start evaluation
    print(f"Starting evaluation{' with test-time augmentation' if use_tta else ''}...")
    start_time = time.time()
    accuracy, preds, labels, probs = evaluate_model(model, test_loader, device, use_tta=use_tta)
    eval_time = time.time() - start_time
    
    print(f"\nEvaluation completed in {eval_time:.2f} seconds")
    print(f"Final Evaluation Accuracy: {accuracy:.2f}%\n")

    # Get class names for visualizations
    target_names = list(DAMAGE_LABELS.keys())
    
    # Check if there are any predictions
    if len(labels) == 0:
        print("No samples were evaluated. Exiting...")
        return

    # Create confusion matrix
    conf_matrix = confusion_matrix(labels, preds)
    print("Confusion Matrix:")
    print(conf_matrix)

    # Create classification report
    if len(labels) > 0 and len(np.unique(labels)) > 0:
        report = classification_report(labels, preds, target_names=target_names, digits=4)
        print("\nClassification Report:")
        print(report)

        # Calculate additional metrics
        macro_f1 = f1_score(labels, preds, average='macro')
        weighted_f1 = f1_score(labels, preds, average='weighted')
        macro_precision = precision_score(labels, preds, average='macro')
        weighted_precision = precision_score(labels, preds, average='weighted')
        macro_recall = recall_score(labels, preds, average='macro')
        weighted_recall = recall_score(labels, preds, average='weighted')
        kappa = cohen_kappa_score(labels, preds)
        bal_accuracy = balanced_accuracy_score(labels, preds)
    else:
        print("No valid labels or predictions to calculate metrics.")
        macro_f1 = weighted_f1 = macro_precision = weighted_precision = macro_recall = weighted_recall = kappa = bal_accuracy = 0.0

    # Print additional metrics
    print("Additional Metrics:")
    print(f"Macro F1 Score: {macro_f1:.4f}")
    print(f"Weighted F1 Score: {weighted_f1:.4f}")
    print(f"Macro Precision: {macro_precision:.4f}")
    print(f"Weighted Precision: {weighted_precision:.4f}")
    print(f"Macro Recall: {macro_recall:.4f}")
    print(f"Weighted Recall: {weighted_recall:.4f}")
    print(f"Cohen's Kappa: {kappa:.4f}")
    print(f"Balanced Accuracy: {bal_accuracy:.4f}")

    # Save detailed results to a text file
    results_path = os.path.join(results_dir, f"evaluation_results_try{try_num}.txt")
    with open(results_path, 'w') as f:
        f.write(f"Evaluation Results (Try {try_num})\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Model: {best_model_path}\n")
        f.write(f"Test-Time Augmentation: {use_tta}\n\n")
        
        if len(labels) > 0:
            f.write("Confusion Matrix:\n")
            f.write(str(conf_matrix) + "\n\n")
            
            if len(np.unique(labels)) > 0:
                f.write("Classification Report:\n")
                f.write(report + "\n\n")
        else:
            f.write("No samples were evaluated.\n\n")
        
        f.write("Additional Metrics:\n")
        f.write(f"Accuracy: {accuracy:.4f}%\n")
        f.write(f"Macro F1 Score: {macro_f1:.4f}\n")
        f.write(f"Weighted F1 Score: {weighted_f1:.4f}\n")
        f.write(f"Macro Precision: {macro_precision:.4f}\n")
        f.write(f"Weighted Precision: {weighted_precision:.4f}\n")
        f.write(f"Macro Recall: {macro_recall:.4f}\n")
        f.write(f"Weighted Recall: {weighted_recall:.4f}\n")
        f.write(f"Cohen's Kappa: {kappa:.4f}\n")
        f.write(f"Balanced Accuracy: {bal_accuracy:.4f}\n")
    
    print(f"Detailed evaluation results saved to {results_path}")

    # Create visualizations 
    cm_save_path = os.path.join(results_dir, "confusion_matrix.png")
    plot_confusion_matrix(conf_matrix, target_names, cm_save_path, 
                         title=f"Confusion Matrix (Accuracy: {accuracy:.2f}%)")
    
    cm_norm_save_path = os.path.join(results_dir, "confusion_matrix_normalized.png")
    plot_confusion_matrix(conf_matrix, target_names, cm_norm_save_path, normalize=True,
                         title="Normalized Confusion Matrix")

    # Plot per-class metrics
    try:
        # Calculate per-class metrics
        per_class_f1 = f1_score(labels, preds, average=None, labels=range(len(target_names)))
        per_class_precision = precision_score(labels, preds, average=None, labels=range(len(target_names)))
        per_class_recall = recall_score(labels, preds, average=None, labels=range(len(target_names)))
        
        # Create plots
        f1_save_path = os.path.join(results_dir, "per_class_f1.png")
        plot_per_class_metrics(per_class_f1, "F1 Score", target_names, f1_save_path)
        
        precision_save_path = os.path.join(results_dir, "per_class_precision.png")
        plot_per_class_metrics(per_class_precision, "Precision", target_names, precision_save_path)
        
        recall_save_path = os.path.join(results_dir, "per_class_recall.png")
        plot_per_class_metrics(per_class_recall, "Recall", target_names, recall_save_path)
        
        # Plot ROC curves for multi-class classification
        roc_save_path = os.path.join(results_dir, "roc_curves.png")
        plot_roc_curves(probs, labels, target_names, roc_save_path)
        
        # Analyze errors
        error_save_path = os.path.join(results_dir, "error_analysis.png")
        analyze_errors(preds, labels, probs, target_names, error_save_path)
    except Exception as e:
        print(f"Error generating metric plots: {e}")
    
    # Randomly sample predictions to show variety
    if len(preds) > 0:
        num_samples = min(15, len(preds))
        sample_indices = random.sample(range(len(preds)), num_samples)
        
        print("\nSample Predictions (randomly selected):")
        print(f"{'Index':<8} {'True Label':<15} {'Predicted':<15} {'Confidence':<10} {'Correct':<8}")
        print("-" * 60)
        
        for i in sample_indices:
            pred_class = target_names[preds[i]]
            true_class = target_names[labels[i]]
            confidence = probs[i, preds[i]]
            correct = "✓" if preds[i] == labels[i] else "✗"
            print(f"{i:<8} {true_class:<15} {pred_class:<15} {confidence:.4f}      {correct}")

if __name__ == "__main__":
    main()







(myenv) pablos@debian:~/Documents/uc3m/Incase$ tree
.
├── data
│   ├── hold              # Unseen data from the challenge
│   │   ├── images
│   │   │   └── … (hold images)
│   │   └── labels
│   │       └── … (hold JSONs)
│   ├── test              # Data for testing your models (flat: images/ and labels/ directly)
│   │   ├── images
│   │   │   └── … (test images: *_pre_disaster.png, *_post_disaster.png)
│   │   └── labels
│   │       └── … (test JSON files)
│   └── xBD               # The core xBD dataset organized by disaster type
│       ├── guatemala-volcano
│       │   ├── images
│       │   │   ├── guatemala-volcano_00000000_pre_disaster.png
│       │   │   ├── guatemala-volcano_00000000_post_disaster.png
│       │   │   └── … (other images)
│       │   └── labels
│       │       ├── guatemala-volcano_00000000_pre_disaster.json
│       │       ├── guatemala-volcano_00000000_post_disaster.json
│       │       └── … (other JSONs)
│       ├── hurricane-florence
│       ├── … etc.
├── evaluation_results
│   ├── evaluationTry1
│   │   ├── confusion_matrix_normalized.png
│   │   ├── confusion_matrix.png
│   │   ├── error_analysis.png
│   │   ├── evaluation_results_try1.txt
│   │   ├── per_class_f1.png
│   │   ├── per_class_precision.png
│   │   ├── per_class_recall.png
│   │   └── roc_curves.png
│   └── evaluationTry2
│       ├── confusion_matrix_normalized.png
│       ├── confusion_matrix.png
│       ├── error_analysis.png
│       ├── evaluation_results_try2.txt
│       ├── per_class_f1.png
│       ├── per_class_precision.png
│       ├── per_class_recall.png
│       └── roc_curves.png
├── FinalScripts.txt
├── importantNotes.txt
├── Literature
│   ├── Areviewofmulticlasschangedetectionforsatellite.pdf
│   ├── Change_Detection_From_Very-High-Spatial-Resolution_Optical_Remote_Sensing_Images_Methods_applications_and_future_directions.pdf
│   ├── Largescaledeeplearningbasedbinary.pdf
│   └── Xview2.pdf
├── output
│   ├── trainingTry1
│   │   ├── config_try1.txt
│   │   ├── models
│   │   │   ├── baseline_best.pt
│   │   │   └── best_model_epoch_33.pt
│   │   ├── pictures
│   │   │   └── learning_curves.png
│   │   └── training_metrics_try1.txt
│   └── trainingTry2
│       ├── config_try4.txt
│       ├── models
│       │   ├── baseline_best.pt
│       │   └── best_model_epoch_19.pt
│       ├── pictures
│       │   └── learning_curves.png
│       └── training_metrics_try4.txt
├── README.md
├── requirements.txt
├── scripts
│   └── training
│       ├── evaluate_baseline.py
│       └── train_baseline.py
├── xview_geotransforms.json
└── xview_tile_centers.geojson

training_metrics:
epochs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
train_losses: [8.95159301194208, 2.026561395200186, 1.09886072993367, 0.856714809348439, 0.701046104870556, 0.6692063206703555, 0.48819640047667584, 0.35824116108810483, 0.2705722299485317, 0.1785369114839702, 0.15362962506365174, 0.101820764566371, 0.07065420245281066, 0.05186611414583559, 0.03481816093493738, 0.02582059516545325, 0.0159516813608815, 0.012145605071058038, 0.009414284982575592, 0.009138920110700709]
val_losses: [1.3052985268033461, 0.6429750904044073, 0.4224016533664623, 0.37198678913938055, 0.6386875022951966, 0.49171912006527374, 0.42945925543931873, 0.42539304566518066, 0.47256443579567003, 0.4283137637939103, 0.4677442713002997, 0.4724981552348867, 0.4734603065087577, 0.5108131662516293, 0.5097999219210282, 0.6083373343471676, 0.6236261550839449, 0.663093121665275, 0.6868827228742799, 0.6449880402272267]
val_accuracies: [44.70583081438647, 50.372366057738645, 57.74521400096377, 67.91299776580365, 53.76965873746003, 53.68423358303763, 53.857274280457354, 59.797608095676175, 49.95181145135147, 64.36675866298681, 63.48622245586367, 72.23463442414685, 74.67910807377228, 80.13755640250581, 77.44556884391291, 85.83694747448197, 85.67923949708678, 87.0788977964691, 87.82801068909625, 87.46221579708241]
per_class_f1_scores:
  Epoch 1: [0.56435038 0.1950884  0.27625074 0.62915055]
  Epoch 2: [0.60480254 0.25258485 0.3649615  0.70168333]
  Epoch 3: [0.67868883 0.28817979 0.58028578 0.6563503 ]
  Epoch 4: [0.78112151 0.3734955  0.59042553 0.64024641]
  Epoch 5: [0.63644283 0.28182152 0.53745595 0.51933044]
  Epoch 6: [0.6321078  0.2853459  0.52112365 0.5667504 ]
  Epoch 7: [0.6329311  0.29101657 0.53204242 0.56849444]
  Epoch 8: [0.69870952 0.32787239 0.56611776 0.58181818]
  Epoch 9: [0.58101642 0.28235399 0.44992327 0.63479884]
  Epoch 10: [0.74603769 0.353762   0.59734396 0.62327295]
  Epoch 11: [0.73785049 0.35021854 0.5990099  0.59653489]
  Epoch 12: [0.82179095 0.40149355 0.60821281 0.68471868]
  Epoch 13: [0.84147081 0.42662188 0.63047333 0.70589598]
  Epoch 14: [0.88574568 0.48095434 0.6746614  0.73185894]
  Epoch 15: [0.86585795 0.45726831 0.62311956 0.71330978]
  Epoch 16: [0.92732053 0.54462009 0.70686398 0.7812979 ]
  Epoch 17: [0.92687573 0.54134723 0.69027611 0.78994006]
  Epoch 18: [0.93584784 0.55772082 0.71140507 0.80398761]
  Epoch 19: [0.94024564 0.56807298 0.71385685 0.80906237]
  Epoch 20: [0.93809205 0.56150277 0.70620521 0.80833562]

evaluation_results:
Evaluation Results (Try 2)
Date: 2025-03-07 10:25:30
Model: /home/pablos/Documents/uc3m/Incase/output/trainingTry4/models/baseline_best.pt
Test-Time Augmentation: True

Confusion Matrix:
[[40562   446   150   269]
 [ 2427  1939   316   116]
 [  826   165  2601   258]
 [  469    58   232  3016]]

Classification Report:
              precision    recall  f1-score   support

   no-damage     0.9160    0.9791    0.9465     41427
minor-damage     0.7435    0.4041    0.5236      4798
major-damage     0.7884    0.6756    0.7277      3850
   destroyed     0.8243    0.7989    0.8114      3775

    accuracy                         0.8936     53850
   macro avg     0.8180    0.7144    0.7523     53850
weighted avg     0.8850    0.8936    0.8837     53850


Additional Metrics:
Accuracy: 89.3556%
Macro F1 Score: 0.7523
Weighted F1 Score: 0.8837
Macro Precision: 0.8180
Weighted Precision: 0.8850
Macro Recall: 0.7144
Weighted Recall: 0.8936
Cohen's Kappa: 0.6992
Balanced Accuracy: 0.7144

Error Analysis:
  Most common misclassification patterns:
  - no-damage → minor-damage (Error rate: 0.02)
  - minor-damage → no-damage (Error rate: 0.60)
  - major-damage → no-damage (Error rate: 0.32)
  - destroyed → no-damage (Error rate: 0.20)

Sample Predictions (randomly selected):
Index    True Label      Predicted       Confidence Correct 
------------------------------------------------------------
41905    no-damage       no-damage       0.9629      ✓
7296     no-damage       no-damage       1.0000      ✓
1639     minor-damage    no-damage       0.7582      ✗
48598    no-damage       no-damage       1.0000      ✓
18024    no-damage       no-damage       1.0000      ✓
16049    no-damage       no-damage       1.0000      ✓
14628    no-damage       no-damage       0.7625      ✓
9144     no-damage       no-damage       0.9990      ✓
48265    no-damage       no-damage       1.0000      ✓
6717     no-damage       no-damage       1.0000      ✓
44348    no-damage       no-damage       0.9564      ✓
48540    no-damage       no-damage       1.0000      ✓
35741    major-damage    major-damage    0.9999      ✓
5697     no-damage       no-damage       0.9999      ✓
38698    no-damage       no-damage       0.9995      ✓